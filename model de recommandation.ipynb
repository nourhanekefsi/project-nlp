{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection des donnees(articles d'actualites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération des articles pour la catégorie : business\n",
      "Traitement de l'URL : https://www.washingtonpost.com/business/2024/12/23/honda-nissan-mitsubishi-merger-negotiations/\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Honda__Nissan_begin_talks_to_forge_world_s_3rd_big.txt\n",
      "Traitement de l'URL : https://www.cnbc.com/2024/12/22/stock-market-today-live-updates.html\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\S_P_500_rises_to_begin_holiday_shortened_week__tec.txt\n",
      "Traitement de l'URL : https://finance.yahoo.com/news/cfpb-sues-walmart-branch-messenger-140953761.html\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\US_government_sues_Walmart__Branch_Messenger_over_.txt\n",
      "Traitement de l'URL : https://www.cnbc.com/2024/12/23/trumps-25percent-tariffs-an-existential-threat-to-canadas-auto-industry.html\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Trump_s_25__tariff_could_be_an_existential_threat_.txt\n",
      "Traitement de l'URL : https://www.yahoo.com/entertainment/did-jeff-bezos-lauren-sanchez-181059106.html\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\What_Did_Jeff_Bezos_Say_About_Lauren_Sanchez_Weddi.txt\n",
      "Traitement de l'URL : https://apnews.com/article/nordstrom-liverpool-acquisition-private-e802942a18e1afd99dcd9857a946b7ce\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Nordstrom_to_be_acquired_by_Nordstrom_family_and_a.txt\n",
      "Traitement de l'URL : https://gizmodo.com/popular-weight-loss-drug-zepbound-gains-fda-approval-to-treat-sleep-apnea-2000542552\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Popular_Weight_Loss_Drug_Zepbound_Gains_FDA_Approv.txt\n",
      "Traitement de l'URL : https://www.axios.com/2024/12/23/consumer-confidence-trump-economy\n",
      "Erreur lors du traitement de https://www.axios.com/2024/12/23/consumer-confidence-trump-economy : Article `download()` failed with 403 Client Error: Forbidden for url: https://www.axios.com/2024/12/23/consumer-confidence-trump-economy on URL https://www.axios.com/2024/12/23/consumer-confidence-trump-economy\n",
      "Traitement de l'URL : https://arstechnica.com/tech-policy/2024/12/chinas-plan-to-dominate-legacy-chips-globally-sparks-us-probe/\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\China_s_plan_to_dominate_legacy_chips_globally_spa.txt\n",
      "Traitement de l'URL : https://www.marketwatch.com/story/stock-market-santa-claus-rally-might-get-a-late-start-this-year-18e58d0b\n",
      "Erreur lors du traitement de https://www.marketwatch.com/story/stock-market-santa-claus-rally-might-get-a-late-start-this-year-18e58d0b : Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.marketwatch.com/story/stock-market-santa-claus-rally-might-get-a-late-start-this-year-18e58d0b on URL https://www.marketwatch.com/story/stock-market-santa-claus-rally-might-get-a-late-start-this-year-18e58d0b\n",
      "Récupération des articles pour la catégorie : science\n",
      "Traitement de l'URL : https://gizmodo.com/watch-nasas-moon-spacecraft-violently-break-apart-during-abort-test-2000542463\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Watch_NASA_s_Moon_Capsule_Violently_Break_Apart_Du.txt\n",
      "Traitement de l'URL : https://dailygalaxy.com/2024/12/worlds-oldest-mammal-ancestor-unearthed-a-saber-toothed-predator-that-preyed-before-dinosaurs/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\World_s_Oldest_Mammal_Ancestor_Unearthed__A__Saber.txt\n",
      "Traitement de l'URL : https://www.earth.com/news/enough-water-to-fill-trillions-of-earths-oceans-found-circling-black-hole-quasar/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Enough_water_to_fill_trillions_of_oceans_found_cir.txt\n",
      "Traitement de l'URL : https://www.livescience.com/space/cosmology/theres-no-real-competitor-theoretical-physicist-marika-taylor-on-how-black-holes-could-help-us-to-find-a-theory-of-everything\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\_There_s_no_real_competitor___Theoretical_physicis.txt\n",
      "Traitement de l'URL : https://www.earth.com/news/buried-water-found-on-mars-sparks-new-hopes-and-headaches/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Buried_water_found_on_Mars_sparks_new_hopes_and_he.txt\n",
      "Traitement de l'URL : https://www.livescience.com/space/watch-chinese-satellite-burn-up-over-us-in-spectacular-fireball\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Watch_Chinese_satellite_burn_up_over_US_in_spectac.txt\n",
      "Traitement de l'URL : https://arstechnica.com/space/2024/12/how-might-nasa-change-under-trump-heres-what-is-being-discussed/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\How_might_NASA_change_under_Trump__Here_s_what_is_.txt\n",
      "Traitement de l'URL : https://www.space.com/space-exploration/artemis/nasa-delays-artemis-missions-again-what-could-this-mean-for-the-moon-mars-and-space-leadership\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\NASA_delays_Artemis_missions_again__What_could_thi.txt\n",
      "Traitement de l'URL : https://phys.org/news/2024-12-earth-sized-exoplanets-orbiting-nearby.html\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Two_Earth_sized_exoplanets_orbiting_nearby_star_de.txt\n",
      "Traitement de l'URL : https://nypost.com/2024/12/23/science/christmas-eve-asteroid-to-make-close-approach-to-earth/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Christmas_Eve_asteroid_the_size_of_a_10_floor_buil.txt\n",
      "Récupération des articles pour la catégorie : sports\n",
      "Traitement de l'URL : https://www.ninersnation.com/2024/12/23/24328191/deebo-samuel-george-kittle-winners-losers-49ers\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\4_Winners_and_2_losers_from_the_49ers_Week_16_loss.txt\n",
      "Traitement de l'URL : https://www.nbcsports.com/nfl/profootballtalk/rumor-mill/news/texans-confirm-tank-dell-tore-his-acl-dislocated-his-kneecap\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Texans_confirm_Tank_Dell_tore_his_ACL__dislocated_.txt\n",
      "Traitement de l'URL : https://www.cbssports.com/nfl/news/nfl-schedule-change-vikings-vs-packers-rematch-gets-bigger-audience-eagles-vs-cowboys-bumped-to-new-slot/\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\NFL_schedule_change__Vikings_vs__Packers_rematch_g.txt\n",
      "Traitement de l'URL : https://uga.rivals.com/news/what-notre-dame-head-coach-marcus-freeman-said-about-georgia-on-monday\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\What_Notre_Dame_head_coach_Marcus_Freeman_said_abo.txt\n",
      "Traitement de l'URL : https://www.elevenwarriors.com/ohio-state-football/2024-25-college-football-playoff/2024/12/152066/presser-bullets-ryan-day-says-theres-a-new-bounce-in-buckeyes-locker-room-after\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Presser_Bullets__Ryan_Day_Says_There_s_a_New__Boun.txt\n",
      "Traitement de l'URL : https://www.espn.com/espn/betting/story/_/id/43126655/2024-nfl-week-17-betting-first-look-odds-lines-picks\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\My_first_bet__Early_picks_for_NFL_Week_17___ESPN.txt\n",
      "Traitement de l'URL : https://www.si.com/nfl/vikings/news/snap-counts-and-notable-pff-grades-from-vikings-clutch-win-over-seahawks-01jft2kryd4d\n",
      "Traitement de l'URL : https://www.cbssports.com/college-football/news/reseeding-the-college-football-playoff-bracket-ohio-state-moves-to-no-2-seed-entering-quarterfinals/\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Reseeding_the_College_Football_Playoff_bracket__Oh.txt\n",
      "Traitement de l'URL : https://apnews.com/article/netflix-christmas-nfl-beyonce-9b65162dbe7e3fe1a2dd13d2da89e966\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Netflix_is_airing_2_NFL_games_on_Christmas_Day__He.txt\n",
      "Traitement de l'URL : https://triblive.com/sports/andrew-mccutchen-re-signs-with-pirates-ahead-of-2025-season/\n",
      "Erreur lors du traitement de https://triblive.com/sports/andrew-mccutchen-re-signs-with-pirates-ahead-of-2025-season/ : Article `download()` failed with 403 Client Error: Forbidden for url: https://triblive.com/sports/andrew-mccutchen-re-signs-with-pirates-ahead-of-2025-season/ on URL https://triblive.com/sports/andrew-mccutchen-re-signs-with-pirates-ahead-of-2025-season/\n",
      "Récupération des articles pour la catégorie : technology\n",
      "Traitement de l'URL : https://www.sfchronicle.com/food/restaurants/article/reems-ferry-building-19998026.php\n",
      "Erreur lors du traitement de https://www.sfchronicle.com/food/restaurants/article/reems-ferry-building-19998026.php : Article `download()` failed with 403 Client Error: Forbidden for url: https://www.sfchronicle.com/food/restaurants/article/reems-ferry-building-19998026.php on URL https://www.sfchronicle.com/food/restaurants/article/reems-ferry-building-19998026.php\n",
      "Traitement de l'URL : https://arstechnica.com/gaming/2024/12/retro-gamers-save-one-of-the-last-45-inch-crt-tvs-in-existence/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\The_quest_to_save_the_world_s_largest_CRT_TV_from_.txt\n",
      "Traitement de l'URL : https://9to5mac.com/2024/12/23/ios-182-just-added-a-faster-way-to-message-siri-and-chatgpt/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\iOS_18_2_just_added_a_faster_way_to_message_Siri_a.txt\n",
      "Traitement de l'URL : https://arstechnica.com/gadgets/2024/12/2100-mechanical-keyboard-has-800-holes-nyc-skyscraper-looks/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\_2_100_mechanical_keyboard_has_800_holes__NYC_skys.txt\n",
      "Traitement de l'URL : https://removed.com\n",
      "Traitement de l'URL : https://www.ksl.com/article/51216304/aspens-to-ashes-this-st-george-artist-turns-wood-burning-into-an-art-form\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Aspens_to_Ashes__This_St__George_artist_turns_wood.txt\n",
      "Traitement de l'URL : https://www.macrumors.com/2024/12/23/what-to-expect-from-ios-19-so-far/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\What_to_Expect_From_iOS_19__All_the_Rumors_So_Far_.txt\n",
      "Traitement de l'URL : https://videocardz.com/newz/asus-confirms-january-6th-launch-date-for-rgb-packed-rog-strix-laptop\n",
      "Erreur lors du traitement de https://videocardz.com/newz/asus-confirms-january-6th-launch-date-for-rgb-packed-rog-strix-laptop : Article `download()` failed with 403 Client Error: Forbidden for url: https://videocardz.com/newz/asus-confirms-january-6th-launch-date-for-rgb-packed-rog-strix-laptop on URL https://videocardz.com/newz/asus-confirms-january-6th-launch-date-for-rgb-packed-rog-strix-laptop\n",
      "Traitement de l'URL : https://www.eurogamer.net/multiversus-pulls-tweet-after-charlie-the-unicorn-creator-says-it-used-his-work-without-permission\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\MultiVersus_pulls_tweet_after_Charlie_the_Unicorn_.txt\n",
      "Traitement de l'URL : https://www.trueachievements.com/news/date-specific-xbox-achievements-holidays\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Unlock_these_15_date_specific_Xbox_achievements_ov.txt\n",
      "Les métadonnées ont été enregistrées dans all_documents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "def fetch_articles(api_key, domains, save_path=\"corpus/article_actualite\", metadata_file=\"all_documents.json\"):\n",
    "    \"\"\"\n",
    "    Récupère des articles à partir d'une API d'actualité, extrait leur contenu avec BeautifulSoup, \n",
    "    et enregistre les métadonnées dans un fichier JSON.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Clé API pour accéder à l'API des actualités.\n",
    "        domains (list): Liste des catégories à récupérer (par ex. ['science', 'sports', 'technology']).\n",
    "        save_path (str): Répertoire où sauvegarder les articles extraits.\n",
    "        metadata_file (str): Fichier JSON pour sauvegarder les métadonnées des articles.\n",
    "    \"\"\"\n",
    "    # Charger les métadonnées existantes ou initialiser une liste vide\n",
    "    if os.path.exists(metadata_file):\n",
    "        with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = []\n",
    "\n",
    "    article_id = len(metadata) + 1  # Commence l'ID à partir du dernier article\n",
    "\n",
    "    base_url = \"https://newsapi.org/v2/top-headlines\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "    for domain in domains:\n",
    "        print(f\"Récupération des articles pour la catégorie : {domain}\")\n",
    "        params = {\"category\": domain, \"language\": \"en\", \"pageSize\": 10}  # Limité à 10 articles par catégorie\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Échec lors de la récupération des articles pour {domain} : {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        articles = response.json().get(\"articles\", [])\n",
    "        domain_path = os.path.join(save_path, domain)\n",
    "        os.makedirs(domain_path, exist_ok=True)\n",
    "\n",
    "        for article in articles:\n",
    "            url = article.get(\"url\")\n",
    "            title = article.get(\"title\", \"Sans titre\")\n",
    "            author = article.get(\"author\", \"Auteur inconnu\")\n",
    "            \n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                print(f\"Traitement de l'URL : {url}\")\n",
    "                if url!=\"https://removed.com\":\n",
    "                    article_text = extract_text_from_url(url)\n",
    "                    if article_text:\n",
    "                        # Enregistrer l'article dans un fichier texte\n",
    "                        file_path = save_article(domain_path, article_text, title)\n",
    "\n",
    "                        # Ajouter les métadonnées au fichier JSON\n",
    "                        metadata.append({\n",
    "                        \"id\": article_id,\n",
    "                        \"title\": title,\n",
    "                        \"author\": author,\n",
    "                        \"type\": \"article d'actualité\",\n",
    "                        \"categorie\": domain,\n",
    "                        \"file_path\": file_path,\n",
    "                        \"url\": url\n",
    "                        })\n",
    "                        article_id += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement de {url} : {e}\")\n",
    "\n",
    "    # Sauvegarder les métadonnées dans le fichier JSON\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Les métadonnées ont été enregistrées dans {metadata_file}\")\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    \"\"\"\n",
    "    Extrait le contenu textuel d'une URL donnée en utilisant Article.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL de l'article.\n",
    "\n",
    "    Returns:\n",
    "        str:titre de l'article,str:auteur de l'article, str: Contenu textuel extrait.\n",
    "    \"\"\"\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    #extraire le contenu \n",
    "    content = article.text\n",
    "    return content\n",
    "\n",
    "\n",
    "def save_article(path, text, title):\n",
    "    \"\"\"\n",
    "    Sauvegarde le contenu de l'article dans un fichier texte.\n",
    "\n",
    "    Args:\n",
    "        path (str): Répertoire où sauvegarder l'article.\n",
    "        text (str): Contenu de l'article.\n",
    "        title (str): Titre de l'article (utilisé comme nom de fichier).\n",
    "\n",
    "    Returns:\n",
    "        str: Chemin complet du fichier sauvegardé.\n",
    "    \"\"\"\n",
    "    safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title)[:50]  # Nettoyer le titre pour en faire un nom de fichier valide\n",
    "    file_path = os.path.join(path, f\"{safe_title}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "    print(f\"Article sauvegardé dans {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"35b99c0ae87b421390320689dee7fda3\"  \n",
    "    DOMAINS = [\"business\", \"science\", \"sports\", \"technology\"]  \n",
    "    fetch_articles(API_KEY, DOMAINS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection de donnes (articles scientifiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les métadonnées des articles ont été enregistrées dans all_documents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from newspaper import Article\n",
    "import feedparser\n",
    "\n",
    "def save_article(keyword_path, content, title):\n",
    "    \"\"\"\n",
    "    Sauvegarde le contenu d'un article dans un fichier texte.\n",
    "\n",
    "    Args:\n",
    "        keyword_path (str): Répertoire où sauvegarder l'article.\n",
    "        content (str): Contenu complet de l'article.\n",
    "        title (str): Titre de l'article.\n",
    "\n",
    "    Returns:\n",
    "        str: Chemin complet du fichier enregistré.\n",
    "    \"\"\"\n",
    "    # Nettoyage du titre pour éviter les caractères non valides dans les noms de fichiers\n",
    "    safe_title = re.sub(r'[^\\w\\s]', '', title).replace(\" \", \"_\")\n",
    "    file_name = f\"{safe_title}.txt\"\n",
    "    file_path = os.path.join(keyword_path, file_name)\n",
    "\n",
    "    # Écrire le contenu complet dans un fichier texte\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    return file_path\n",
    "\n",
    "def fetch_article_with_newspaper(article_url):\n",
    "    try:\n",
    "        # Utilisation de newspaper3k pour extraire le contenu de l'article\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec Newspaper : {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_scientific_articles(keywords, save_path=\"corpus/articles_presses\", metadata_file=\"all_documents.json\"):\n",
    "    \"\"\"\n",
    "    Récupère des articles de presse à partir de mots-clés, extrait leur contenu complet, et enregistre les métadonnées.\n",
    "\n",
    "    Args:\n",
    "        keywords (list): Liste de mots-clés pour filtrer les articles (par ex. ['AI', 'biology', 'climate']).\n",
    "        save_path (str): Répertoire où sauvegarder les articles extraits.\n",
    "        metadata_file (str): Fichier JSON pour sauvegarder les métadonnées des articles extraits.\n",
    "    \"\"\"\n",
    "    # Charger ou initialiser les métadonnées\n",
    "    if os.path.exists(metadata_file):\n",
    "        with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = []\n",
    "\n",
    "    article_id = len(metadata) + 1  # ID unique pour chaque article\n",
    "\n",
    "    # URL du flux RSS (exemple ici avec le New York Times)\n",
    "    feed_url = \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"\n",
    "    \n",
    "    # Récupérer et analyser le flux RSS\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    \n",
    "    # Parcourir chaque article du flux RSS\n",
    "    for entry in feed.entries:\n",
    "        title = entry.title\n",
    "        summary = entry.summary  # Utilisation de 'summary' ou 'content' si disponible\n",
    "        url = entry.link\n",
    "\n",
    "        if not url:\n",
    "            print(f\"Article {title} ignoré : URL manquant.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Récupérer le contenu complet de l'article\n",
    "            full_content = fetch_article_with_newspaper(url)\n",
    "            if not full_content:\n",
    "                full_content = summary  # Si le contenu complet est introuvable, utiliser le résumé\n",
    "\n",
    "            # Sauvegarder l'article\n",
    "            keyword = 'general'  # Exemple de catégorie\n",
    "            keyword_path = os.path.join(save_path, keyword)\n",
    "            os.makedirs(keyword_path, exist_ok=True)\n",
    "            \n",
    "            file_path = save_article(keyword_path, full_content, title)\n",
    "\n",
    "            metadata.append({\n",
    "                \"id\": article_id,\n",
    "                \"title\": title,\n",
    "                \"type\": \"article de presse\",\n",
    "                \"categorie\": keyword,\n",
    "                \"file_path\": file_path,\n",
    "                \"url\": url\n",
    "            })\n",
    "            article_id += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'enregistrement de {title} : {e}\")\n",
    "\n",
    "    # Sauvegarder les métadonnées dans un fichier JSON\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Les métadonnées des articles ont été enregistrées dans {metadata_file}\")\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    KEYWORDS = [\"tech\", \"biology\", \"climate change\", \"quantum computing\"]\n",
    "    fetch_scientific_articles(KEYWORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing des docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les contenus prétraités et le vocabulaire ont été enregistrés dans le fichier : preprocessed_content.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import collections\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Charger le tokenizer de DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def read_documents(json_file_path):\n",
    "    \"\"\"\n",
    "    Lit le fichier JSON, extrait les ids et les chemins des fichiers, lit le contenu de chaque fichier,\n",
    "    et retourne un dictionnaire associant chaque id à son contenu.\n",
    "\n",
    "    :param json_file_path: Chemin du fichier JSON contenant les métadonnées des documents (id, chemin du fichier).\n",
    "    :return: Un dictionnaire {id: contenu du document}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "            documents = json.load(json_file)\n",
    "        \n",
    "        doc_contents = {}\n",
    "        for doc in documents:\n",
    "            doc_id = doc.get(\"id\")\n",
    "            file_path = doc.get(\"file_path\")\n",
    "            # Vérification de l'existence du fichier et des informations nécessaires\n",
    "            if doc_id and file_path and os.path.exists(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                doc_contents[doc_id] = content\n",
    "            else:\n",
    "                print(f\"Fichier introuvable ou informations manquantes pour l'id {doc_id}: {file_path}\")\n",
    "        return doc_contents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la lecture des documents : {e}\")\n",
    "        return {}\n",
    "\n",
    "def preprocess_documents(documents_content):\n",
    "    \"\"\"\n",
    "    Applique le prétraitement aux documents :\n",
    "    - Tokenisation (utilisation de DistilBERT)\n",
    "    - Construction du vocabulaire global à partir des tokens\n",
    "\n",
    "    :param documents_content: Dictionnaire {id: contenu brut des documents}.\n",
    "    :return: Dictionnaire avec le contenu prétraité et le vocabulaire global sous forme de Counter.\n",
    "    \"\"\"\n",
    "    vocabulary = collections.Counter()\n",
    "    preprocessed_content = {}\n",
    "\n",
    "    for doc_id, content in documents_content.items():\n",
    "        # Tokenisation du contenu du document avec gestion de la longueur des séquences\n",
    "        tokens = tokenizer(content, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        # Enregistrer les tokens sous forme de dictionnaire avec input_ids et attention_mask\n",
    "        preprocessed_content[doc_id] = {\n",
    "            'input_ids': tokens['input_ids'].squeeze().tolist(),  # Convertir en liste simple\n",
    "            'attention_mask': tokens['attention_mask'].squeeze().tolist()  # Convertir en liste simple\n",
    "        }\n",
    "        \n",
    "        # Mise à jour du vocabulaire global\n",
    "        vocabulary.update(tokens['input_ids'].squeeze().tolist())\n",
    "\n",
    "    return preprocessed_content, vocabulary\n",
    "\n",
    "def save_preprocessed_data(preprocessed_content, vocabulary, output_file=\"preprocessed_content.json\"):\n",
    "    \"\"\"\n",
    "    Sauvegarde le contenu prétraité et le vocabulaire dans un fichier JSON.\n",
    "\n",
    "    :param preprocessed_content: Dictionnaire {id: contenu prétraité} (tokens).\n",
    "    :param vocabulary: Vocabulaire global sous forme de `collections.Counter` (liste des ids).\n",
    "    :param output_file: Chemin du fichier de sortie pour enregistrer les données.\n",
    "    \"\"\"\n",
    "    data_to_save = {\n",
    "        \"preprocessed_content\": preprocessed_content,\n",
    "        \"vocabulary\": list(vocabulary.keys())  # Utiliser les clés du Counter comme vocabulaire\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data_to_save, json_file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Les contenus prétraités et le vocabulaire ont été enregistrés dans le fichier : {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la sauvegarde des données : {e}\")\n",
    "\n",
    "# Application principale\n",
    "if __name__ == \"__main__\":\n",
    "    json_file_path = \"all_documents.json\"\n",
    "    documents_content = read_documents(json_file_path)\n",
    "\n",
    "    if documents_content:\n",
    "        preprocessed_content, vocabulary = preprocess_documents(documents_content)\n",
    "        save_preprocessed_data(preprocessed_content, vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation des documents sous forme de vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les vecteurs DistilBERT des documents ont été enregistrés dans le fichier distilbert_vectors.json.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Charger le modèle et le tokenizer DistilBERT\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def load_preprocessed_content(input_file=\"preprocessed_content.json\"):\n",
    "    \"\"\"\n",
    "    Charge le contenu prétraité et le vocabulaire depuis un fichier JSON.\n",
    "\n",
    "    :param input_file: Nom du fichier JSON contenant les données prétraitées et le vocabulaire.\n",
    "    :return: Tuple (preprocessed_content, vocabulary) contenant les données prétraitées et le vocabulaire.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        preprocessed_content = data.get(\"preprocessed_content\", {})\n",
    "        vocabulary = data.get(\"vocabulary\", [])\n",
    "        return preprocessed_content, vocabulary\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur : Le fichier {input_file} n'a pas été trouvé.\")\n",
    "        return {}, []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Erreur : Le fichier {input_file} n'est pas un fichier JSON valide.\")\n",
    "        return {}, []\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la lecture du fichier : {e}\")\n",
    "        return {}, []\n",
    "\n",
    "def create_distilbert_embeddings(doc_contents, output_file=\"distilbert_vectors.json\"):\n",
    "    \"\"\"\n",
    "    Utilise DistilBERT pour calculer des embeddings pour chaque document en moyennant les embeddings des tokens.\n",
    "\n",
    "    :param doc_contents: Dictionnaire {id: contenu prétraité des documents (tokens)}.\n",
    "    :param output_file: Nom du fichier JSON où les embeddings seront enregistrés.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not doc_contents:\n",
    "            print(\"Erreur : Aucun contenu de document fourni pour calculer les embeddings.\")\n",
    "            return\n",
    "\n",
    "        document_vectors = {}\n",
    "\n",
    "        for doc_id, tokens in doc_contents.items():\n",
    "            # Vérification que tokens est bien un dictionnaire avec 'input_ids' et 'attention_mask'\n",
    "            input_ids = tokens['input_ids']\n",
    "            attention_mask = tokens['attention_mask']\n",
    "\n",
    "            # Calcul des embeddings avec le modèle DistilBERT\n",
    "            with torch.no_grad():\n",
    "                # Assurez-vous que les tokens contiennent input_ids et attention_mask\n",
    "                outputs = model(input_ids=torch.tensor([input_ids]),  # Ajouter une dimension batch\n",
    "                                attention_mask=torch.tensor([attention_mask]))  # Ajouter une dimension batch\n",
    "                embeddings = outputs.last_hidden_state.mean(dim=1)  # Moyenne des embeddings des tokens\n",
    "\n",
    "            # Ajout des embeddings du document au dictionnaire\n",
    "            document_vectors[doc_id] = embeddings.squeeze().tolist()\n",
    "\n",
    "        # Enregistrer les vecteurs dans un fichier JSON\n",
    "        with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(document_vectors, json_file, indent=4)\n",
    "\n",
    "        print(f\"Les vecteurs DistilBERT des documents ont été enregistrés dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors du calcul des embeddings : {e}\")\n",
    "\n",
    "# Application principale\n",
    "if __name__ == \"__main__\":\n",
    "    # Charger les contenus prétraités\n",
    "    preprocessed_content, vocabulary = load_preprocessed_content()\n",
    "\n",
    "    # Vérifier si des contenus ont été chargés avec succès\n",
    "    if preprocessed_content:\n",
    "        # Créer les vecteurs avec DistilBERT\n",
    "        create_distilbert_embeddings(preprocessed_content, output_file=\"distilbert_vectors.json\")\n",
    "    else:\n",
    "        print(\"Impossible de générer les vecteurs car aucun contenu prétraité n'a été chargé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le calcule de similarites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrice de similarité a été enregistrée dans le fichier document_similarity_matrix.csv.\n",
      "Les similarités triées ont été enregistrées dans le fichier sorted_document_similarities.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def compute_similarity_matrix(vector_file=\"distilbert_vectors.json\", output_file=\"document_similarity_matrix.csv\"):\n",
    "    \"\"\"\n",
    "    Calcule la similarité cosinus entre tous les documents et enregistre les résultats dans un fichier CSV sous forme de matrice.\n",
    "\n",
    "    :param vector_file: Nom du fichier JSON contenant les vecteurs des documents.\n",
    "    :param output_file: Nom du fichier CSV pour enregistrer la matrice des similarités.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger les vecteurs depuis le fichier JSON\n",
    "        with open(vector_file, 'r', encoding='utf-8') as json_file:\n",
    "            document_vectors = json.load(json_file)\n",
    "        \n",
    "        # Extraire les IDs et les vecteurs\n",
    "        doc_ids = list(document_vectors.keys())\n",
    "        vectors = np.array([document_vectors[doc_id] for doc_id in doc_ids])\n",
    "        \n",
    "        # Calculer la matrice de similarité cosinus\n",
    "        similarity_matrix = cosine_similarity(vectors)\n",
    "        \n",
    "        # Enregistrer la matrice dans un fichier CSV\n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Écrire l'en-tête (IDs des documents)\n",
    "            writer.writerow([\"\"] + doc_ids)\n",
    "            \n",
    "            # Écrire chaque ligne (ID + similarités)\n",
    "            for i, doc_id in enumerate(doc_ids):\n",
    "                writer.writerow([doc_id] + similarity_matrix[i].tolist())\n",
    "        \n",
    "        print(f\"La matrice de similarité a été enregistrée dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "\n",
    "def save_sorted_similarities_from_matrix(matrix_file=\"document_similarity_matrix.csv\", output_file=\"sorted_document_similarities.csv\"):\n",
    "    \"\"\"\n",
    "    Enregistre dans un fichier CSV les IDs des documents les plus similaires pour chaque document,\n",
    "    triés par similarité décroissante, à partir d'une matrice de similarité déjà calculée.\n",
    "\n",
    "    :param matrix_file: Nom du fichier CSV contenant la matrice de similarité.\n",
    "    :param output_file: Nom du fichier CSV pour enregistrer les similarités triées (seulement les IDs).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger la matrice de similarité depuis le fichier CSV\n",
    "        with open(matrix_file, 'r', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            rows = list(reader)\n",
    "        \n",
    "        # Extraire les IDs des documents (en-tête)\n",
    "        doc_ids = rows[0][1:]\n",
    "        \n",
    "        # Préparer les similarités triées (seulement les IDs)\n",
    "        sorted_similarities = []\n",
    "        for i, row in enumerate(rows[1:]):\n",
    "            doc_id = row[0]\n",
    "            similarities = [(doc_ids[j], float(row[j + 1])) for j in range(len(doc_ids)) if i != j]\n",
    "            \n",
    "            # Trier les similarités par ordre décroissant et récupérer uniquement les IDs\n",
    "            sorted_doc_ids = [sim_doc_id for sim_doc_id, _ in sorted(similarities, key=lambda x: x[1], reverse=True)]\n",
    "            sorted_similarities.append((doc_id, sorted_doc_ids))\n",
    "        \n",
    "        # Enregistrer les IDs triés dans un fichier CSV\n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Écrire l'en-tête (Document + Liste d'IDs des documents similaires)\n",
    "            header = [\"Document\"] + [f\"Similar Doc {i+1}\" for i in range(len(doc_ids) - 1)]\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            # Écrire les similarités triées pour chaque document\n",
    "            for doc_id, sorted_doc_ids in sorted_similarities:\n",
    "                # Compléter avec des colonnes vides si moins de documents similaires\n",
    "                row = [doc_id] + sorted_doc_ids + [''] * (len(doc_ids) - 1 - len(sorted_doc_ids))\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"Les similarités triées ont été enregistrées dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    compute_similarity_matrix(vector_file=\"distilbert_vectors.json\", output_file=\"document_similarity_matrix.csv\")\n",
    "    save_sorted_similarities_from_matrix(\n",
    "        matrix_file=\"document_similarity_matrix.csv\",\n",
    "        output_file=\"sorted_document_similarities.csv\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
