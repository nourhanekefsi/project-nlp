{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection des donnees(articles d'actualites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération des articles pour la catégorie : business\n",
      "Traitement de l'URL : https://apnews.com/article/openai-whistleblower-suchir-balaji-death-283e70b31d34ebb71b62e73aafb56a7d\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Ex_OpenAI_engineer_who_raised_legal_concerns_about.txt\n",
      "Traitement de l'URL : https://techcrunch.com/2024/12/21/openais-gpt-5-reportedly-falling-short-of-expectations/\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\OpenAI_s_GPT_5_reportedly_falling_short_of_expecta.txt\n",
      "Traitement de l'URL : https://removed.com\n",
      "Traitement de l'URL : https://www.cnn.com/2024/12/21/business/teamsters-amazon-strike/index.html\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Teamsters_expand_strike_against_Amazon___CNN.txt\n",
      "Traitement de l'URL : https://www.forbes.com/sites/tylerroush/2024/12/21/mega-millions-jackpot-reaches-944-million-for-christmas-eve-drawing-heres-how-much-the-winner-could-take-home-after-taxes/\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Mega_Millions_Jackpot_Reaches__944_Million_For_Chr.txt\n",
      "Traitement de l'URL : https://www.wsj.com/business/americas-farm-recession-is-here-one-early-response-is-sending-billions-to-farmers-9fc42981\n",
      "Erreur lors du traitement de https://www.wsj.com/business/americas-farm-recession-is-here-one-early-response-is-sending-billions-to-farmers-9fc42981 : Échec lors de la récupération de l'URL : https://www.wsj.com/business/americas-farm-recession-is-here-one-early-response-is-sending-billions-to-farmers-9fc42981\n",
      "Traitement de l'URL : https://www.marketwatch.com/story/forget-the-stock-market-tumble-the-fed-made-the-right-move-in-a-wild-week-1a73f20b\n",
      "Erreur lors du traitement de https://www.marketwatch.com/story/forget-the-stock-market-tumble-the-fed-made-the-right-move-in-a-wild-week-1a73f20b : Échec lors de la récupération de l'URL : https://www.marketwatch.com/story/forget-the-stock-market-tumble-the-fed-made-the-right-move-in-a-wild-week-1a73f20b\n",
      "Traitement de l'URL : https://www.fool.com/investing/2024/12/21/could-this-artificial-intelligence-ai-stock-be-the/\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Could_This_Artificial_Intelligence__AI__Stock_Be_t.txt\n",
      "Traitement de l'URL : https://www.businessinsider.com/careers-regrets-retirement-unemployment-boomers-college-degree-age-discrimination-ageism-2024-12\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Skipping_college_and_switching_jobs__What_older_Am.txt\n",
      "Traitement de l'URL : https://www.washingtonpost.com/business/2024/12/20/big-lots-set-liquidate-its-900-stores-after-planned-sale-falters/\n",
      "Erreur lors du traitement de https://www.washingtonpost.com/business/2024/12/20/big-lots-set-liquidate-its-900-stores-after-planned-sale-falters/ : ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Récupération des articles pour la catégorie : science\n",
      "Traitement de l'URL : https://phys.org/news/2024-12-genomic-journey-modern-archaic-humans.html\n",
      "Erreur lors du traitement de https://phys.org/news/2024-12-genomic-journey-modern-archaic-humans.html : Échec lors de la récupération de l'URL : https://phys.org/news/2024-12-genomic-journey-modern-archaic-humans.html\n",
      "Traitement de l'URL : https://www.cnn.com/2024/12/21/science/ursid-meteor-shower-december-2024/index.html\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\How_to_see_the_Ursids__the_final_meteor_shower_tha.txt\n",
      "Traitement de l'URL : https://www.thebrighterside.news/post/the-universe-might-actually-contain-3-time-dimensions-and-only-1-space-dimension/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\The_universe_might_actually_contain_3_time_dimensi.txt\n",
      "Traitement de l'URL : https://phys.org/news/2024-12-scientists-negative-quantum.html\n",
      "Erreur lors du traitement de https://phys.org/news/2024-12-scientists-negative-quantum.html : Échec lors de la récupération de l'URL : https://phys.org/news/2024-12-scientists-negative-quantum.html\n",
      "Traitement de l'URL : https://www.livescience.com/animals/reptiles/why-do-iguanas-fall-from-trees-in-florida\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Why_do_iguanas_fall_from_trees_in_Florida____Lives.txt\n",
      "Traitement de l'URL : https://arstechnica.com/science/2024/12/scientists-explore-frozen-scars-and-fiery-sands-on-the-seafloor/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Scientists_explore_frozen_scars_and_fiery_sands_on.txt\n",
      "Traitement de l'URL : https://gizmodo.com/nasa-plans-for-continuous-heartbeat-in-space-after-iss-retirement-2000539863\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\NASA_Plans_for__Continuous_Heartbeat__in_Space_Aft.txt\n",
      "Traitement de l'URL : https://www.salon.com/2024/12/21/nasas-webb-telescope-seemingly-confirms-controversial-theory-on-planet-formation/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\NASA_s_James_Webb_telescope_confirms_planetary_the.txt\n",
      "Traitement de l'URL : https://www.wired.com/story/parker-solar-probe-atmosphere/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\A_Spacecraft_Is_About_to_Fly_Into_the_Sun_s_Atmosp.txt\n",
      "Traitement de l'URL : https://mashable.com/article/mars-catalog-man-made-material-junk-trash\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Mars_is_littered_with_junk__Historians_want_to_sav.txt\n",
      "Récupération des articles pour la catégorie : sports\n",
      "Traitement de l'URL : https://www.nbcsports.com/nfl/profootballtalk/rumor-mill/news/tank-dell-suffers-apparent-serious-knee-injury-on-touchdown-catch\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Tank_Dell_suffers_apparent_serious_knee_injury_on_.txt\n",
      "Traitement de l'URL : https://www.dazn.com/en-US/news/boxing/oleksandr-usyk-vs-tyson-fury-2-live-fight-updates/j09hrwa3gewr1n8jltldhnk7x\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Usyk_vs_Fury_2_LIVE__Huge_undercard_underway___Joh.txt\n",
      "Traitement de l'URL : https://nypost.com/2024/12/21/sports/rickey-henderson-baseball-legend-dead-at-65/\n",
      "Erreur lors du traitement de https://nypost.com/2024/12/21/sports/rickey-henderson-baseball-legend-dead-at-65/ : Échec lors de la récupération de l'URL : https://nypost.com/2024/12/21/sports/rickey-henderson-baseball-legend-dead-at-65/\n",
      "Traitement de l'URL : https://www.nbcsports.com/nfl/profootballtalk/rumor-mill/news/whats-next-for-diontae-johnson\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\What_s_next_for_Diontae_Johnson____NBC_Sports.txt\n",
      "Traitement de l'URL : https://www.cbssports.com/nfl/news/nfl-week-16-odds-predictions-expert-picks-best-bets-teasers-survivor-picks-where-to-watch-saturday-games/\n",
      "Erreur lors du traitement de https://www.cbssports.com/nfl/news/nfl-week-16-odds-predictions-expert-picks-best-bets-teasers-survivor-picks-where-to-watch-saturday-games/ : Échec lors de la récupération de l'URL : https://www.cbssports.com/nfl/news/nfl-week-16-odds-predictions-expert-picks-best-bets-teasers-survivor-picks-where-to-watch-saturday-games/\n",
      "Traitement de l'URL : https://www.espn.com/soccer/story/_/id/43094109/bukayo-saka-arsenal-winger-forced-injured-crystal-palace\n",
      "Erreur lors du traitement de https://www.espn.com/soccer/story/_/id/43094109/bukayo-saka-arsenal-winger-forced-injured-crystal-palace : Échec lors de la récupération de l'URL : https://www.espn.com/soccer/story/_/id/43094109/bukayo-saka-arsenal-winger-forced-injured-crystal-palace\n",
      "Traitement de l'URL : https://www.cbssports.com/college-football/news/penn-state-vs-smu-score-live-game-updates-college-football-playoff-scores-first-round-coverage/live/\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Penn_State_vs__SMU_score__Live_game_updates__Colle.txt\n",
      "Traitement de l'URL : https://nypost.com/2024/12/21/sports/rangers-matt-rempe-offered-in-person-hearing-for-hit-on-miro-heiskanen/\n",
      "Erreur lors du traitement de https://nypost.com/2024/12/21/sports/rangers-matt-rempe-offered-in-person-hearing-for-hit-on-miro-heiskanen/ : Échec lors de la récupération de l'URL : https://nypost.com/2024/12/21/sports/rangers-matt-rempe-offered-in-person-hearing-for-hit-on-miro-heiskanen/\n",
      "Traitement de l'URL : https://removed.com\n",
      "Traitement de l'URL : https://badgerofhonor.com/luke-fickell-and-the-badgers-turned-down-option-to-fill-in-as-bowl-game-replacement\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Luke_Fickell_and_the_Badgers_Turned_Down_Option_to.txt\n",
      "Récupération des articles pour la catégorie : technology\n",
      "Traitement de l'URL : https://www.androidcentral.com/phones/the-google-pixel-9-pro-fold-suddenly-scores-a-shocking-discount-at-amazon-and-yes-itll-arrive-by-christmas\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\The_Google_Pixel_9_Pro_Fold_suddenly_scores_a_shoc.txt\n",
      "Traitement de l'URL : https://thegamepost.com/sony-absorbs-bungie-strategic-team-playstation-ip/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Sony_Absorbs_More_Of_Bungie__Strategic_Partnership.txt\n",
      "Traitement de l'URL : https://www.theverge.com/2024/12/21/24326557/asus-copilot-plus-nuc-mini-pc-intel-core-ultra-series-2-processor\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Here_s_the_first_CoPilot_plus_mini_PC_with_Intel_s.txt\n",
      "Traitement de l'URL : https://www.cnn.com/2024/12/21/science/artificial-intelligence-ai-science-2024/index.html\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\_A_sort_of_superpower___Unexpected_revelations_mad.txt\n",
      "Traitement de l'URL : https://www.nintendolife.com/news/2024/12/sega-is-evaluating-its-own-netflix-style-subscription-service\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Sega_Is__Evaluating__Its_Own_Netflix_Style_Subscri.txt\n",
      "Traitement de l'URL : https://www.theverge.com/2024/12/21/24326032/lg-b4-series-oled-tv-apple-ipad-10th-gen-deal-sale\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\LG_s_brilliant_B4_OLED_TV_is_on_sale_with_a__50_gi.txt\n",
      "Traitement de l'URL : https://www.washingtonpost.com/technology/2024/12/21/kids-music-devices/\n",
      "Erreur lors du traitement de https://www.washingtonpost.com/technology/2024/12/21/kids-music-devices/ : ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Traitement de l'URL : https://wccftech.com/early-pricing-nvidia-geforce-rtx-5090-rtx-5080-based-acer-predator-pcs/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Early_Pricing_Revealed_For_NVIDIA_GeForce_RTX_5090.txt\n",
      "Traitement de l'URL : https://www.forbes.com/sites/daveywinder/2024/12/21/apple-warns-users-of-iphone-spyware-attacks-what-you-need-to-know/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Apple_Warns_Users_Of_iPhone_Spyware_Attacks_What_Y.txt\n",
      "Traitement de l'URL : https://www.zdnet.com/article/zdnets-product-of-the-year-why-oura-ring-4-bested-samsung-apple-and-others-in-2024/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\ZDNET_s_product_of_the_year__Why_Oura_Ring_4_beste.txt\n",
      "Les métadonnées ont été enregistrées dans all_documents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_articles(api_key, domains, save_path=\"corpus/article_actualite\", metadata_file=\"all_documents.json\"):\n",
    "    \"\"\"\n",
    "    Récupère des articles à partir d'une API d'actualité, extrait leur contenu avec BeautifulSoup, \n",
    "    et enregistre les métadonnées dans un fichier JSON.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Clé API pour accéder à l'API des actualités.\n",
    "        domains (list): Liste des catégories à récupérer (par ex. ['science', 'sports', 'technology']).\n",
    "        save_path (str): Répertoire où sauvegarder les articles extraits.\n",
    "        metadata_file (str): Fichier JSON pour sauvegarder les métadonnées des articles.\n",
    "    \"\"\"\n",
    "    # Charger les métadonnées existantes ou initialiser une liste vide\n",
    "    if os.path.exists(metadata_file):\n",
    "        with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = []\n",
    "\n",
    "    article_id = len(metadata) + 1  # Commence l'ID à partir du dernier article\n",
    "\n",
    "    base_url = \"https://newsapi.org/v2/top-headlines\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "    for domain in domains:\n",
    "        print(f\"Récupération des articles pour la catégorie : {domain}\")\n",
    "        params = {\"category\": domain, \"language\": \"en\", \"pageSize\": 10}  # Limité à 10 articles par catégorie\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Échec lors de la récupération des articles pour {domain} : {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        articles = response.json().get(\"articles\", [])\n",
    "        domain_path = os.path.join(save_path, domain)\n",
    "        os.makedirs(domain_path, exist_ok=True)\n",
    "\n",
    "        for article in articles:\n",
    "            url = article.get(\"url\")\n",
    "            title = article.get(\"title\", \"Sans titre\")\n",
    "            author = article.get(\"author\", \"Auteur inconnu\")\n",
    "            \n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                print(f\"Traitement de l'URL : {url}\")\n",
    "                if url!=\"https://removed.com\":\n",
    "                    article_text = extract_text_from_url(url)\n",
    "                    if article_text:\n",
    "                        # Enregistrer l'article dans un fichier texte\n",
    "                        file_path = save_article(domain_path, article_text, title)\n",
    "\n",
    "                        # Ajouter les métadonnées au fichier JSON\n",
    "                        metadata.append({\n",
    "                        \"id\": article_id,\n",
    "                        \"title\": title,\n",
    "                        \"author\": author,\n",
    "                        \"type\": \"article d'actualité\",\n",
    "                        \"categorie\": domain,\n",
    "                        \"file_path\": file_path\n",
    "                        })\n",
    "                        article_id += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement de {url} : {e}\")\n",
    "\n",
    "    # Sauvegarder les métadonnées dans le fichier JSON\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Les métadonnées ont été enregistrées dans {metadata_file}\")\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    \"\"\"\n",
    "    Extrait le contenu textuel d'une URL donnée en utilisant BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL de l'article.\n",
    "\n",
    "    Returns:\n",
    "        str: Contenu textuel extrait.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Échec lors de la récupération de l'URL : {url}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \"\\n\".join([para.get_text() for para in paragraphs if para.get_text()])\n",
    "    return text\n",
    "\n",
    "def save_article(path, text, title):\n",
    "    \"\"\"\n",
    "    Sauvegarde le contenu de l'article dans un fichier texte.\n",
    "\n",
    "    Args:\n",
    "        path (str): Répertoire où sauvegarder l'article.\n",
    "        text (str): Contenu de l'article.\n",
    "        title (str): Titre de l'article (utilisé comme nom de fichier).\n",
    "\n",
    "    Returns:\n",
    "        str: Chemin complet du fichier sauvegardé.\n",
    "    \"\"\"\n",
    "    safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title)[:50]  # Nettoyer le titre pour en faire un nom de fichier valide\n",
    "    file_path = os.path.join(path, f\"{safe_title}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "    print(f\"Article sauvegardé dans {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"35b99c0ae87b421390320689dee7fda3\"  \n",
    "    DOMAINS = [\"business\", \"science\", \"sports\", \"technology\"]  \n",
    "    fetch_articles(API_KEY, DOMAINS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection de donnes (articles scientifiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération des articles scientifiques pour le mot-clé : tech\n",
      "Enregistrement de l'article : The Contestation of Tech Ethics: A Sociotechnical Approach to Technology\n",
      "  Ethics in Practice\n",
      "Erreur lors de l'enregistrement de The Contestation of Tech Ethics: A Sociotechnical Approach to Technology\n",
      "  Ethics in Practice : [Errno 22] Invalid argument: 'corpus/scientific_articles\\\\tech\\\\The_Contestation_of_Tech_Ethics_A_Sociotechnical_Approach_to_Technology\\n__Ethics_in_Practice.txt'\n",
      "Enregistrement de l'article : Exploring Multi-Dimensional Events Characterizing Tech Start-Up\n",
      "  Emergence in the Nigerian Entrepreneurial Ecosystem\n",
      "Erreur lors de l'enregistrement de Exploring Multi-Dimensional Events Characterizing Tech Start-Up\n",
      "  Emergence in the Nigerian Entrepreneurial Ecosystem : [Errno 22] Invalid argument: 'corpus/scientific_articles\\\\tech\\\\Exploring_MultiDimensional_Events_Characterizing_Tech_StartUp\\n__Emergence_in_the_Nigerian_Entrepreneurial_Ecosystem.txt'\n",
      "Enregistrement de l'article : Constraints on R-parity Violation from Precision Electroweak\n",
      "  Measurements\n",
      "Erreur lors de l'enregistrement de Constraints on R-parity Violation from Precision Electroweak\n",
      "  Measurements : [Errno 22] Invalid argument: 'corpus/scientific_articles\\\\tech\\\\Constraints_on_Rparity_Violation_from_Precision_Electroweak\\n__Measurements.txt'\n",
      "Enregistrement de l'article : Women Want to Learn Tech: Lessons from the Czechitas Education Project\n",
      "Enregistrement de l'article : A Review of Research on Civic Technology: Definitions, Theories, History\n",
      "  and Insights\n",
      "Erreur lors de l'enregistrement de A Review of Research on Civic Technology: Definitions, Theories, History\n",
      "  and Insights : [Errno 22] Invalid argument: 'corpus/scientific_articles\\\\tech\\\\A_Review_of_Research_on_Civic_Technology_Definitions_Theories_History\\n__and_Insights.txt'\n",
      "Récupération des articles scientifiques pour le mot-clé : biology\n",
      "Enregistrement de l'article : Borges Dilemma, Fundamental Laws, and Systems Biology\n",
      "Enregistrement de l'article : Quantum Biology at the Cellular Level - elements of the research program\n",
      "Enregistrement de l'article : Landscape Paradigms in Physics and Biology: Introduction and Overview\n",
      "Enregistrement de l'article : Mathematics at the eve of a historic transition in biology\n",
      "Enregistrement de l'article : G-quadruplexes and mRNA localization\n",
      "Récupération des articles scientifiques pour le mot-clé : climate change\n",
      "Enregistrement de l'article : The structure of the climate debate\n",
      "Enregistrement de l'article : Baumol's Climate Disease\n",
      "Enregistrement de l'article : You are right. I am ALARMED -- But by Climate Change Counter Movement\n",
      "Enregistrement de l'article : Climate Change Conspiracy Theories on Social Media\n",
      "Enregistrement de l'article : Hurricanes Increase Climate Change Conversations on Twitter\n",
      "Récupération des articles scientifiques pour le mot-clé : quantum computing\n",
      "Enregistrement de l'article : The Rise of Quantum Internet Computing\n",
      "Enregistrement de l'article : Unconventional Quantum Computing Devices\n",
      "Enregistrement de l'article : Geometrical perspective on quantum states and quantum computation\n",
      "Enregistrement de l'article : Quantum Computation and Quantum Information\n",
      "Enregistrement de l'article : Google Quantum AI's Quest for Error-Corrected Quantum Computers\n",
      "Les métadonnées des articles scientifiques ont été enregistrées dans all_documents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def save_article(keyword_path, abstract, title):\n",
    "    \"\"\"\n",
    "    Sauvegarde le contenu d'un article scientifique dans un fichier texte.\n",
    "\n",
    "    Args:\n",
    "        keyword_path (str): Répertoire où sauvegarder l'article.\n",
    "        abstract (str): Résumé de l'article.\n",
    "        title (str): Titre de l'article.\n",
    "\n",
    "    Returns:\n",
    "        str: Chemin complet du fichier enregistré.\n",
    "    \"\"\"\n",
    "    # Nettoyage du titre pour éviter les caractères non valides dans les noms de fichiers\n",
    "    safe_title = re.sub(r'[^\\w\\s]', '', title).replace(\" \", \"_\")\n",
    "    file_name = f\"{safe_title}.txt\"\n",
    "    file_path = os.path.join(keyword_path, file_name)\n",
    "\n",
    "    # Écrire le résumé dans un fichier texte\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(abstract)\n",
    "\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def fetch_scientific_articles(keywords, save_path=\"corpus/scientific_articles\", metadata_file=\"all_documents.json\"):\n",
    "    \"\"\"\n",
    "    Récupère des articles scientifiques courts à partir de l'API ArXiv, extrait leur contenu, et enregistre les métadonnées.\n",
    "\n",
    "    Args:\n",
    "        keywords (list): Liste de mots-clés pour filtrer les articles (par ex. ['AI', 'biology', 'climate']).\n",
    "        save_path (str): Répertoire où sauvegarder les articles scientifiques extraits.\n",
    "        metadata_file (str): Fichier JSON pour sauvegarder les métadonnées des articles scientifiques.\n",
    "    \"\"\"\n",
    "    # Charger ou initialiser les métadonnées\n",
    "    if os.path.exists(metadata_file):\n",
    "        with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = []\n",
    "\n",
    "    article_id = len(metadata) + 1  # ID unique pour chaque article\n",
    "\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "    for keyword in keywords:\n",
    "        print(f\"Récupération des articles scientifiques pour le mot-clé : {keyword}\")\n",
    "        params = {\n",
    "            \"search_query\": f\"all:{keyword}\",\n",
    "            \"start\": 0,\n",
    "            \"max_results\": 5\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Échec lors de la récupération des articles pour {keyword} : {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        # Parser la réponse XML avec BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"xml\")\n",
    "        entries = soup.find_all(\"entry\")\n",
    "\n",
    "        keyword_path = os.path.join(save_path, keyword)\n",
    "        os.makedirs(keyword_path, exist_ok=True)\n",
    "\n",
    "        for article in entries:\n",
    "            title = article.find(\"title\").get_text(strip=True)\n",
    "            authors = \", \".join([author.find(\"name\").get_text(strip=True) for author in article.find_all(\"author\")])\n",
    "            abstract = article.find(\"summary\").get_text(strip=True)\n",
    "            paper_url = article.find(\"id\").get_text(strip=True)\n",
    "\n",
    "            if not abstract or not paper_url:\n",
    "                print(f\"Article {title} ignoré : Résumé ou URL manquant.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                print(f\"Enregistrement de l'article : {title}\")\n",
    "                file_path = save_article(keyword_path, abstract, title)\n",
    "\n",
    "                metadata.append({\n",
    "                    \"id\": article_id,\n",
    "                    \"title\": title,\n",
    "                    \"author\": authors or \"Auteur inconnu\",\n",
    "                    \"type\": \"article scientifique\",\n",
    "                    \"categorie\": keyword,\n",
    "                    \"file_path\": file_path\n",
    "                })\n",
    "                article_id += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'enregistrement de {title} : {e}\")\n",
    "\n",
    "    # Sauvegarder les métadonnées dans un fichier JSON\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Les métadonnées des articles scientifiques ont été enregistrées dans {metadata_file}\")\n",
    "\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    KEYWORDS = [\"tech\", \"biology\", \"climate change\", \"quantum computing\"]\n",
    "    fetch_scientific_articles(KEYWORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing des docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les contenus prétraités et le vocabulaire ont été enregistrés dans le fichier : preprocessed_content.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "\n",
    "# Assurez-vous que les ressources nécessaires sont téléchargées\n",
    "import nltk\n",
    "\n",
    "def read_documents(json_file_path):\n",
    "    \"\"\"\n",
    "    Lit le fichier JSON, extrait les ids et les chemins des fichiers, lit le contenu de chaque fichier,\n",
    "    et retourne un dictionnaire associant chaque id à son contenu.\n",
    "\n",
    "    :param json_file_path: Chemin du fichier JSON contenant les métadonnées.\n",
    "    :return: Un dictionnaire {id: contenu}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "            documents = json.load(json_file)\n",
    "        \n",
    "        doc_contents = {}\n",
    "        for doc in documents:\n",
    "            doc_id = doc.get(\"id\")\n",
    "            file_path = doc.get(\"file_path\")\n",
    "            if doc_id and file_path and os.path.exists(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                doc_contents[doc_id] = content\n",
    "            else:\n",
    "                print(f\"Fichier introuvable ou informations manquantes pour l'id {doc_id}: {file_path}\")\n",
    "        return doc_contents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la lecture des documents : {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalise le texte en supprimant les accents et les caractères spéciaux.\n",
    "\n",
    "    :param text: Chaîne de texte à normaliser.\n",
    "    :return: Texte normalisé.\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # Supprime les caractères non alphabétiques\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Supprime les espaces multiples\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_documents(documents_content):\n",
    "    \"\"\"\n",
    "    Applique le prétraitement aux documents :\n",
    "    - Normalisation (suppression des accents, caractères spéciaux)\n",
    "    - Tokenisation\n",
    "    - Suppression des stopwords\n",
    "    - Conversion en minuscules\n",
    "    - Lemmatisation\n",
    "\n",
    "    :param documents_content: Dictionnaire {id: contenu brut}.\n",
    "    :return: Contenu prétraité et vocabulaire global.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    vocabulary = collections.Counter()\n",
    "\n",
    "    preprocessed_content = {}\n",
    "\n",
    "    for doc_id, content in documents_content.items():\n",
    "        # Normalisation du texte\n",
    "        content = normalize_text(content)\n",
    "\n",
    "        # Tokenisation\n",
    "        tokens = word_tokenize(content)\n",
    "\n",
    "        # Suppression des stopwords et conversion en minuscules\n",
    "        tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "        # Lemmatisation\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # Mise à jour des résultats\n",
    "        preprocessed_content[doc_id] = tokens\n",
    "        vocabulary.update(tokens)\n",
    "\n",
    "    return preprocessed_content, vocabulary\n",
    "\n",
    "\n",
    "def save_preprocessed_data(preprocessed_content, vocabulary, output_file=\"preprocessed_content.json\"):\n",
    "    \"\"\"\n",
    "    Sauvegarde le contenu prétraité et le vocabulaire dans un fichier JSON.\n",
    "\n",
    "    :param preprocessed_content: Dictionnaire {id: contenu prétraité}.\n",
    "    :param vocabulary: Vocabulaire global sous forme de `collections.Counter`.\n",
    "    :param output_file: Chemin du fichier de sortie.\n",
    "    \"\"\"\n",
    "    data_to_save = {\n",
    "        \"preprocessed_content\": preprocessed_content,\n",
    "        \"vocabulary\": list(vocabulary.keys())\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data_to_save, json_file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Les contenus prétraités et le vocabulaire ont été enregistrés dans le fichier : {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la sauvegarde des données : {e}\")\n",
    "\n",
    "\n",
    "# Application principale\n",
    "if __name__ == \"__main__\":\n",
    "    json_file_path = \"all_documents.json\"\n",
    "    documents_content = read_documents(json_file_path)\n",
    "\n",
    "    if documents_content:\n",
    "        preprocessed_content, vocabulary = preprocess_documents(documents_content)\n",
    "        save_preprocessed_data(preprocessed_content, vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation des documents sous forme de vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle Word2Vec a été sauvegardé sous le nom 'word2vec_model.model'.\n",
      "Les vecteurs des documents ont été enregistrés dans le fichier document_vectors.json.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_preprocessed_content(input_file=\"preprocessed_content.json\"):\n",
    "    \"\"\"\n",
    "    Charge le contenu prétraité et le vocabulaire depuis un fichier JSON.\n",
    "\n",
    "    :param input_file: Nom du fichier JSON contenant les données.\n",
    "    :return: Tuple (preprocessed_content, vocabulary).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        \n",
    "        preprocessed_content = data.get(\"preprocessed_content\", {})\n",
    "        vocabulary = data.get(\"vocabulary\", [])\n",
    "        return preprocessed_content, vocabulary\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur : Le fichier {input_file} n'a pas été trouvé.\")\n",
    "        return {}, []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Erreur : Le fichier {input_file} n'est pas un fichier JSON valide.\")\n",
    "        return {}, []\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la lecture du fichier : {e}\")\n",
    "        return {}, []\n",
    "\n",
    "def create_tfidf_model(doc_contents, output_file=\"tfidf_vectors.json\"):\n",
    "    \"\"\"\n",
    "    Calcule les vecteurs TF-IDF pour chaque document et les enregistre dans un fichier JSON.\n",
    "\n",
    "    :param doc_contents: Dictionnaire {id: contenu du document (liste de mots)}.\n",
    "    :param output_file: Nom du fichier pour enregistrer les vecteurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Vérification si doc_contents est vide\n",
    "        if not doc_contents:\n",
    "            print(\"Erreur : Aucun contenu de document fourni pour TF-IDF.\")\n",
    "            return\n",
    "\n",
    "        # Convertir les documents en format texte\n",
    "        documents = [\" \".join(words) for words in doc_contents.values()]\n",
    "        \n",
    "        # Initialiser le vectoriseur TF-IDF\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "        # Récupérer les vecteurs des documents sous forme de dictionnaire\n",
    "        document_vectors = {\n",
    "            doc_id: tfidf_matrix[i].toarray().flatten().tolist()\n",
    "            for i, doc_id in enumerate(doc_contents.keys())\n",
    "        }\n",
    "\n",
    "        # Enregistrer les vecteurs dans un fichier JSON\n",
    "        with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(document_vectors, json_file, indent=4)\n",
    "\n",
    "        print(f\"Les vecteurs TF-IDF des documents ont été enregistrés dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "def create_word2vec_model_with_spacy(doc_contents, output_file=\"word2vec_vectors.json\", model_name=\"en_core_web_md\"):\n",
    "    \"\"\"\n",
    "    Utilise un modèle préentraîné Spacy pour calculer les vecteurs moyens pour chaque document.\n",
    "\n",
    "    :param doc_contents: Dictionnaire {id: contenu du document (liste de mots)}.\n",
    "    :param output_file: Nom du fichier pour enregistrer les vecteurs.\n",
    "    :param model_name: Nom du modèle Spacy préentraîné à charger.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Vérification si doc_contents est vide\n",
    "        if not doc_contents:\n",
    "            print(\"Erreur : Aucun contenu de document fourni pour entraîner le modèle.\")\n",
    "            return\n",
    "\n",
    "        # Charger le modèle Spacy\n",
    "        nlp = spacy.load(model_name)\n",
    "        print(f\"Modèle Spacy préentraîné '{model_name}' chargé avec succès.\")\n",
    "\n",
    "        # Calculer les vecteurs des documents (moyenne des vecteurs de mots)\n",
    "        document_vectors = {}\n",
    "        for doc_id, words in doc_contents.items():\n",
    "            text = \" \".join(words)\n",
    "            doc = nlp(text)\n",
    "            if doc.vector_norm != 0:\n",
    "                document_vectors[doc_id] = doc.vector.tolist()\n",
    "            else:\n",
    "                document_vectors[doc_id] = [0] * nlp.vector_size  # Cas où le document est vide ou non représentable\n",
    "\n",
    "        # Enregistrer les vecteurs dans un fichier JSON\n",
    "        with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(document_vectors, json_file, indent=4)\n",
    "        \n",
    "        print(f\"Les vecteurs des documents ont été enregistrés dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    # Charger les contenus prétraités\n",
    "    preprocessed_content, vocabulary = load_preprocessed_content()\n",
    "    \n",
    "    # Vérifier si des contenus ont été chargés avec succès\n",
    "    if preprocessed_content:\n",
    "        # Créer les vecteurs TF-IDF\n",
    "        create_tfidf_model(preprocessed_content, output_file=\"tfidf_vectors.json\")\n",
    "\n",
    "        # Créer les vecteurs Word2Vec en utilisant Spacy\n",
    "        create_word2vec_model_with_spacy(preprocessed_content, output_file=\"word2vec_vectors.json\", model_name=\"en_core_web_md\")\n",
    "    else:\n",
    "        print(\"Impossible de générer les vecteurs car aucun contenu prétraité n'a été chargé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le calcule de similarites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrice de similarité a été enregistrée dans le fichier document_similarity_matrix.csv.\n",
      "Les similarités triées ont été enregistrées dans le fichier sorted_document_similarities.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def compute_similarity_matrix(vector_file=\"document_vectors.json\", output_file=\"document_similarity_matrix.csv\"):\n",
    "    \"\"\"\n",
    "    Calcule la similarité cosinus entre tous les documents et enregistre les résultats dans un fichier CSV sous forme de matrice.\n",
    "\n",
    "    :param vector_file: Nom du fichier JSON contenant les vecteurs des documents.\n",
    "    :param output_file: Nom du fichier CSV pour enregistrer la matrice des similarités.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger les vecteurs depuis le fichier JSON\n",
    "        with open(vector_file, 'r', encoding='utf-8') as json_file:\n",
    "            document_vectors = json.load(json_file)\n",
    "        \n",
    "        # Extraire les IDs et les vecteurs\n",
    "        doc_ids = list(document_vectors.keys())\n",
    "        vectors = np.array([document_vectors[doc_id] for doc_id in doc_ids])\n",
    "        \n",
    "        # Calculer la matrice de similarité cosinus\n",
    "        similarity_matrix = cosine_similarity(vectors)\n",
    "        \n",
    "        # Enregistrer la matrice dans un fichier CSV\n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Écrire l'en-tête (IDs des documents)\n",
    "            writer.writerow([\"\"] + doc_ids)\n",
    "            \n",
    "            # Écrire chaque ligne (ID + similarités)\n",
    "            for i, doc_id in enumerate(doc_ids):\n",
    "                writer.writerow([doc_id] + similarity_matrix[i].tolist())\n",
    "        \n",
    "        print(f\"La matrice de similarité a été enregistrée dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "import csv\n",
    "\n",
    "def save_sorted_similarities_from_matrix(matrix_file=\"document_similarity_matrix.csv\", output_file=\"sorted_document_similarities.csv\"):\n",
    "    \"\"\"\n",
    "    Enregistre dans un fichier CSV les IDs des documents les plus similaires pour chaque document,\n",
    "    triés par similarité décroissante, à partir d'une matrice de similarité déjà calculée.\n",
    "\n",
    "    :param matrix_file: Nom du fichier CSV contenant la matrice de similarité.\n",
    "    :param output_file: Nom du fichier CSV pour enregistrer les similarités triées (seulement les IDs).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger la matrice de similarité depuis le fichier CSV\n",
    "        with open(matrix_file, 'r', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            rows = list(reader)\n",
    "        \n",
    "        # Extraire les IDs des documents (en-tête)\n",
    "        doc_ids = rows[0][1:]\n",
    "        \n",
    "        # Préparer les similarités triées (seulement les IDs)\n",
    "        sorted_similarities = []\n",
    "        for i, row in enumerate(rows[1:]):\n",
    "            doc_id = row[0]\n",
    "            similarities = [(doc_ids[j], float(row[j + 1])) for j in range(len(doc_ids)) if i != j]\n",
    "            \n",
    "            # Trier les similarités par ordre décroissant et récupérer uniquement les IDs\n",
    "            sorted_doc_ids = [sim_doc_id for sim_doc_id, _ in sorted(similarities, key=lambda x: x[1], reverse=True)]\n",
    "            sorted_similarities.append((doc_id, sorted_doc_ids))\n",
    "        \n",
    "        # Enregistrer les IDs triés dans un fichier CSV\n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Écrire l'en-tête (Document + Liste d'IDs des documents similaires)\n",
    "            header = [\"Document\"] + [f\"Similar Doc {i+1}\" for i in range(len(doc_ids) - 1)]\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            # Écrire les similarités triées pour chaque document\n",
    "            for doc_id, sorted_doc_ids in sorted_similarities:\n",
    "                # Compléter avec des colonnes vides si moins de documents similaires\n",
    "                row = [doc_id] + sorted_doc_ids + [''] * (len(doc_ids) - 1 - len(sorted_doc_ids))\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"Les similarités triées ont été enregistrées dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    compute_similarity_matrix(vector_file=\"document_vectors.json\", output_file=\"document_similarity_matrix.csv\")\n",
    "    save_sorted_similarities_from_matrix(\n",
    "        matrix_file=\"document_similarity_matrix.csv\",\n",
    "        output_file=\"sorted_document_similarities.csv\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
