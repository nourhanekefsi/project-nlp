{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection des donnees(articles d'actualites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération des articles pour la catégorie : business\n",
      "Traitement de l'URL : https://www.theverge.com/2024/12/23/24328046/walmart-spark-delivery-lawsuit-branch-instant-payment\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Walmart_sued_over_illegally_opening_bank_accounts_.txt\n",
      "Traitement de l'URL : https://www.marketwatch.com/story/stock-market-santa-claus-rally-might-get-a-late-start-this-year-18e58d0b\n",
      "Erreur lors du traitement de https://www.marketwatch.com/story/stock-market-santa-claus-rally-might-get-a-late-start-this-year-18e58d0b : Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.marketwatch.com/story/stock-market-santa-claus-rally-might-get-a-late-start-this-year-18e58d0b on URL https://www.marketwatch.com/story/stock-market-santa-claus-rally-might-get-a-late-start-this-year-18e58d0b\n",
      "Traitement de l'URL : https://www.washingtonpost.com/technology/2024/12/23/arizona-data-centers-navajo-power-aps-srp/\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\In_the_shadows_of_Arizona_s_data_center_boom__thou.txt\n",
      "Traitement de l'URL : https://finance.yahoo.com/news/rosenblatt-picks-amd-meta-top-163311433.html\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Rosenblatt_Picks_AMD_and_Meta_as_Top_Stocks_for_20.txt\n",
      "Traitement de l'URL : https://www.wane.com/top-stories/two-million-dollar-mega-millions-lottery-tickets-sold-in-indiana/\n",
      "Erreur lors du traitement de https://www.wane.com/top-stories/two-million-dollar-mega-millions-lottery-tickets-sold-in-indiana/ : Article `download()` failed with 429 Client Error: Unknown Error for url: https://www.wane.com/top-stories/two-million-dollar-mega-millions-lottery-tickets-sold-in-indiana/ on URL https://www.wane.com/top-stories/two-million-dollar-mega-millions-lottery-tickets-sold-in-indiana/\n",
      "Traitement de l'URL : https://komonews.com/news/local/nordstrom-to-be-acquired-by-nordstrom-family-and-a-mexican-retail-group-in-625-billion-seattle\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Nordstrom_to_be_acquired_by_Nordstrom_family__Mexi.txt\n",
      "Traitement de l'URL : https://techcrunch.com/2024/12/23/honda-and-nissan-plan-major-merger-focused-on-intelligence-and-electrification/\n",
      "Article sauvegardé dans corpus/article_actualite\\business\\Honda_and_Nissan_plan_major_merger_focused_on__int.txt\n",
      "Traitement de l'URL : https://www.wsj.com/economy/u-s-consumers-feel-less-confident-as-economy-concerns-mount-82635510\n",
      "Erreur lors du traitement de https://www.wsj.com/economy/u-s-consumers-feel-less-confident-as-economy-concerns-mount-82635510 : Article `download()` failed with 403 Client Error: Forbidden for url: https://www.wsj.com/economy/u-s-consumers-feel-less-confident-as-economy-concerns-mount-82635510 on URL https://www.wsj.com/economy/u-s-consumers-feel-less-confident-as-economy-concerns-mount-82635510\n",
      "Traitement de l'URL : https://www.barrons.com/articles/bitcoin-price-dropping-04eef03a\n",
      "Erreur lors du traitement de https://www.barrons.com/articles/bitcoin-price-dropping-04eef03a : Article `download()` failed with 403 Client Error: Forbidden for url: https://www.barrons.com/articles/bitcoin-price-dropping-04eef03a on URL https://www.barrons.com/articles/bitcoin-price-dropping-04eef03a\n",
      "Traitement de l'URL : https://www.investors.com/news/technology/arm-stock-qualcomm-stock-diverge-lawsuit-verdict/\n",
      "Erreur lors du traitement de https://www.investors.com/news/technology/arm-stock-qualcomm-stock-diverge-lawsuit-verdict/ : Article `download()` failed with 403 Client Error: Forbidden for url: https://www.investors.com/news/technology/arm-stock-qualcomm-stock-diverge-lawsuit-verdict/ on URL https://www.investors.com/news/technology/arm-stock-qualcomm-stock-diverge-lawsuit-verdict/\n",
      "Récupération des articles pour la catégorie : science\n",
      "Traitement de l'URL : https://www.earth.com/news/enough-water-to-fill-trillions-of-earths-oceans-found-circling-black-hole-quasar/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Enough_water_to_fill_trillions_of_oceans_found_cir.txt\n",
      "Traitement de l'URL : https://www.earth.com/news/buried-water-found-on-mars-sparks-new-hopes-and-headaches/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Buried_water_found_on_Mars_sparks_new_hopes_and_he.txt\n",
      "Traitement de l'URL : https://timesofindia.indiatimes.com/world/us/watch-fireballs-light-up-us-sky-as-chinese-satellite-burns-up-in-atmosphere/articleshow/116605385.cms\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Watch___Fireballs__light_up_US_sky_as_Chinese_sate.txt\n",
      "Traitement de l'URL : https://arstechnica.com/space/2024/12/how-might-nasa-change-under-trump-heres-what-is-being-discussed/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\How_might_NASA_change_under_Trump__Here_s_what_is_.txt\n",
      "Traitement de l'URL : https://www.space.com/space-exploration/artemis/nasa-delays-artemis-missions-again-what-could-this-mean-for-the-moon-mars-and-space-leadership\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\NASA_delays_Artemis_missions_again__What_could_thi.txt\n",
      "Traitement de l'URL : https://nypost.com/2024/12/23/science/christmas-eve-asteroid-to-make-close-approach-to-earth/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Christmas_Eve_asteroid_the_size_of_a_10_floor_buil.txt\n",
      "Traitement de l'URL : https://dailygalaxy.com/2024/12/james-webb-hybrid-asteroid-and-comet/\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\James_Webb_Telescope_Uncovers_a_Mind_Bending_Hybri.txt\n",
      "Traitement de l'URL : https://gizmodo.com/nasas-solar-probe-will-make-history-on-christmas-eve-by-zooming-closer-to-the-sun-than-ever-2000541644\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\NASA_s_Solar_Probe_Will_Make_History_on_Christmas_.txt\n",
      "Traitement de l'URL : https://www.livescience.com/technology/engineering/lasers-powered-by-sunlight-could-beam-energy-through-space-to-support-interplanetary-missions\n",
      "Article sauvegardé dans corpus/article_actualite\\science\\Lasers_powered_by_sunlight_could_beam_energy_throu.txt\n",
      "Traitement de l'URL : https://www.forbes.com/sites/jamiecartereurope/2024/12/23/a-comet-for-christmas-see-2024s-spectacular-comet-dazzle-one-final-time/\n",
      "Erreur lors du traitement de https://www.forbes.com/sites/jamiecartereurope/2024/12/23/a-comet-for-christmas-see-2024s-spectacular-comet-dazzle-one-final-time/ : Article `download()` failed with 403 Client Error: Max restarts limit reached for url: https://www.forbes.com/sites/jamiecartereurope/2024/12/23/a-comet-for-christmas-see-2024s-spectacular-comet-dazzle-one-final-time/ on URL https://www.forbes.com/sites/jamiecartereurope/2024/12/23/a-comet-for-christmas-see-2024s-spectacular-comet-dazzle-one-final-time/\n",
      "Récupération des articles pour la catégorie : sports\n",
      "Traitement de l'URL : https://www.nbcsports.com/fantasy/football/news/sunday-aftermath-hurts-concussion-kenneth-walkers-latest-injury-and-much-more\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Sunday_Aftermath__Hurts__concussion__Kenneth_Walke.txt\n",
      "Traitement de l'URL : https://www.si.com/nfl/the-revival-of-joe-burrows-bengals\n",
      "Traitement de l'URL : https://nypost.com/2024/12/23/sports/mets-nearing-billion-dollar-offseason-with-sean-manaea-contract/\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Steve_Cohen_nearing__1_billion_in_offseason_spendi.txt\n",
      "Traitement de l'URL : https://www.pff.com/news/nfl-week-16-pff-team-of-the-week-player-awards-2024\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\NFL_Week_16__PFF_Team_of_the_Week___Player_Awards_.txt\n",
      "Traitement de l'URL : https://www.cbssports.com/college-football/news/reseeding-the-college-football-playoff-bracket-ohio-state-moves-to-no-2-seed-entering-quarterfinals/\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Reseeding_the_College_Football_Playoff_bracket__Oh.txt\n",
      "Traitement de l'URL : https://www.cnn.com/2024/12/23/sport/nfl-week-16-sunday-review-spt-intl/index.html\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Historic_Daniels_performance_snaps_Eagles_streak__.txt\n",
      "Traitement de l'URL : https://www.cbssports.com/college-basketball/news/college-basketball-rankings-why-tennessee-deserves-to-be-ranked-no-1-over-auburn/\n",
      "Erreur lors du traitement de https://www.cbssports.com/college-basketball/news/college-basketball-rankings-why-tennessee-deserves-to-be-ranked-no-1-over-auburn/ : Article `download()` failed with HTTPSConnectionPool(host='www.cbssports.com', port=443): Read timed out. (read timeout=7) on URL https://www.cbssports.com/college-basketball/news/college-basketball-rankings-why-tennessee-deserves-to-be-ranked-no-1-over-auburn/\n",
      "Traitement de l'URL : https://www.mlbtraderumors.com/2024/12/red-sox-to-sign-walker-buehler.html\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Red_Sox_To_Sign_Walker_Buehler___MLB_Trade_Rumors.txt\n",
      "Traitement de l'URL : https://www.mmafighting.com/2024/12/23/24327203/jiri-prochazka-jamahal-hill-trying-to-build-a-confidence-where-there-is-not-confidence-trash-talk\n",
      "Article sauvegardé dans corpus/article_actualite\\sports\\Jiri_Prochazka__Jamahal_Hill_trying__to_build_a_co.txt\n",
      "Traitement de l'URL : https://www.si.com/sports-illustrated/the-best-sports-days-of-2024\n",
      "Récupération des articles pour la catégorie : technology\n",
      "Traitement de l'URL : https://arstechnica.com/gadgets/2024/12/2100-mechanical-keyboard-has-800-holes-nyc-skyscraper-looks/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\_2_100_mechanical_keyboard_has_800_holes__NYC_skys.txt\n",
      "Traitement de l'URL : https://www.theverge.com/2024/12/23/24327980/sony-wh-1000xm4-noise-canceling-headphones-rig-nacon-revolution-x-controller-deal-sale\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Sony_s_WH_1000XM4_headphones_are_nearly_50_percent.txt\n",
      "Traitement de l'URL : http://9to5google.com/2024/12/23/samsung-2025-foldable-production-report/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Report__Samsung_slashes_2025_foldable_plans_after_.txt\n",
      "Traitement de l'URL : https://9to5mac.com/2024/12/23/new-feature-back-to-the-mac-in-2025/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\This_one_new_feature_might_finally_bring_me_back_t.txt\n",
      "Traitement de l'URL : https://videocardz.com/newz/asus-confirms-january-6th-launch-date-for-rgb-packed-rog-strix-laptop\n",
      "Erreur lors du traitement de https://videocardz.com/newz/asus-confirms-january-6th-launch-date-for-rgb-packed-rog-strix-laptop : Article `download()` failed with 403 Client Error: Forbidden for url: https://videocardz.com/newz/asus-confirms-january-6th-launch-date-for-rgb-packed-rog-strix-laptop on URL https://videocardz.com/newz/asus-confirms-january-6th-launch-date-for-rgb-packed-rog-strix-laptop\n",
      "Traitement de l'URL : https://www.eurogamer.net/multiversus-pulls-tweet-after-charlie-the-unicorn-creator-says-it-used-his-work-without-permission\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\MultiVersus_pulls_tweet_after_Charlie_the_Unicorn_.txt\n",
      "Traitement de l'URL : https://www.trueachievements.com/news/date-specific-xbox-achievements-holidays\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Unlock_these_15_date_specific_Xbox_achievements_ov.txt\n",
      "Traitement de l'URL : https://www.notateslaapp.com/news/2446/tesla-addresses-cybertruck-tonneau-cover-leaks-with-new-rubber-seals\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Tesla_Addresses_Cybertruck_Tonneau_Cover_Leaks_wit.txt\n",
      "Traitement de l'URL : http://9to5google.com/2024/12/23/apple-rcs-iphone-expansion-google-fi-waiting/\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Apple_expands_RCS_support_on_iPhone_to_more_carrie.txt\n",
      "Traitement de l'URL : https://www.theverge.com/24326185/amazon-kindle-paperwhite-signature-edition-2024-e-reader-review\n",
      "Article sauvegardé dans corpus/article_actualite\\technology\\Amazon_Kindle_Paperwhite__2024__review__slightly_l.txt\n",
      "Les métadonnées ont été enregistrées dans all_documents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "def fetch_articles(api_key, domains, save_path=\"corpus/article_actualite\", metadata_file=\"all_documents.json\"):\n",
    "    \"\"\"\n",
    "    Récupère des articles à partir d'une API d'actualité, extrait leur contenu avec BeautifulSoup, \n",
    "    et enregistre les métadonnées dans un fichier JSON.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Clé API pour accéder à l'API des actualités.\n",
    "        domains (list): Liste des catégories à récupérer (par ex. ['science', 'sports', 'technology']).\n",
    "        save_path (str): Répertoire où sauvegarder les articles extraits.\n",
    "        metadata_file (str): Fichier JSON pour sauvegarder les métadonnées des articles.\n",
    "    \"\"\"\n",
    "    # Charger les métadonnées existantes ou initialiser une liste vide\n",
    "    if os.path.exists(metadata_file):\n",
    "        with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = []\n",
    "\n",
    "    article_id = len(metadata) + 1  # Commence l'ID à partir du dernier article\n",
    "\n",
    "    base_url = \"https://newsapi.org/v2/top-headlines\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "    for domain in domains:\n",
    "        print(f\"Récupération des articles pour la catégorie : {domain}\")\n",
    "        params = {\"category\": domain, \"language\": \"en\", \"pageSize\": 10}  # Limité à 10 articles par catégorie\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Échec lors de la récupération des articles pour {domain} : {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        articles = response.json().get(\"articles\", [])\n",
    "        domain_path = os.path.join(save_path, domain)\n",
    "        os.makedirs(domain_path, exist_ok=True)\n",
    "\n",
    "        for article in articles:\n",
    "            url = article.get(\"url\")\n",
    "            title = article.get(\"title\", \"Sans titre\")\n",
    "            author = article.get(\"author\", \"Auteur inconnu\")\n",
    "            \n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                print(f\"Traitement de l'URL : {url}\")\n",
    "                if url!=\"https://removed.com\":\n",
    "                    article_text = extract_text_from_url(url)\n",
    "                    if article_text:\n",
    "                        # Enregistrer l'article dans un fichier texte\n",
    "                        file_path = save_article(domain_path, article_text, title)\n",
    "\n",
    "                        # Ajouter les métadonnées au fichier JSON\n",
    "                        metadata.append({\n",
    "                        \"id\": article_id,\n",
    "                        \"title\": title,\n",
    "                        \"author\": author,\n",
    "                        \"type\": \"article d'actualité\",\n",
    "                        \"categorie\": domain,\n",
    "                        \"file_path\": file_path,\n",
    "                        \"url\": url\n",
    "                        })\n",
    "                        article_id += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du traitement de {url} : {e}\")\n",
    "\n",
    "    # Sauvegarder les métadonnées dans le fichier JSON\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Les métadonnées ont été enregistrées dans {metadata_file}\")\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    \"\"\"\n",
    "    Extrait le contenu textuel d'une URL donnée en utilisant Article.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL de l'article.\n",
    "\n",
    "    Returns:\n",
    "        str:titre de l'article,str:auteur de l'article, str: Contenu textuel extrait.\n",
    "    \"\"\"\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    #extraire le contenu \n",
    "    content = article.text\n",
    "    return content\n",
    "\n",
    "\n",
    "def save_article(path, text, title):\n",
    "    \"\"\"\n",
    "    Sauvegarde le contenu de l'article dans un fichier texte.\n",
    "\n",
    "    Args:\n",
    "        path (str): Répertoire où sauvegarder l'article.\n",
    "        text (str): Contenu de l'article.\n",
    "        title (str): Titre de l'article (utilisé comme nom de fichier).\n",
    "\n",
    "    Returns:\n",
    "        str: Chemin complet du fichier sauvegardé.\n",
    "    \"\"\"\n",
    "    safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title)[:50]  # Nettoyer le titre pour en faire un nom de fichier valide\n",
    "    file_path = os.path.join(path, f\"{safe_title}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "    print(f\"Article sauvegardé dans {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"35b99c0ae87b421390320689dee7fda3\"  \n",
    "    DOMAINS = [\"business\", \"science\", \"sports\", \"technology\"]  \n",
    "    fetch_articles(API_KEY, DOMAINS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection de donnes (articles scientifiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération des articles scientifiques pour le mot-clé : tech\n",
      "Enregistrement de l'article : The Contestation of Tech Ethics: A Sociotechnical Approach to Technology\n",
      "  Ethics in Practice\n",
      "Erreur lors de l'enregistrement de The Contestation of Tech Ethics: A Sociotechnical Approach to Technology\n",
      "  Ethics in Practice : [Errno 22] Invalid argument: 'corpus/scientific_articles\\\\tech\\\\The_Contestation_of_Tech_Ethics_A_Sociotechnical_Approach_to_Technology\\n__Ethics_in_Practice.txt'\n",
      "Enregistrement de l'article : Exploring Multi-Dimensional Events Characterizing Tech Start-Up\n",
      "  Emergence in the Nigerian Entrepreneurial Ecosystem\n",
      "Erreur lors de l'enregistrement de Exploring Multi-Dimensional Events Characterizing Tech Start-Up\n",
      "  Emergence in the Nigerian Entrepreneurial Ecosystem : [Errno 22] Invalid argument: 'corpus/scientific_articles\\\\tech\\\\Exploring_MultiDimensional_Events_Characterizing_Tech_StartUp\\n__Emergence_in_the_Nigerian_Entrepreneurial_Ecosystem.txt'\n",
      "Enregistrement de l'article : Constraints on R-parity Violation from Precision Electroweak\n",
      "  Measurements\n",
      "Erreur lors de l'enregistrement de Constraints on R-parity Violation from Precision Electroweak\n",
      "  Measurements : [Errno 22] Invalid argument: 'corpus/scientific_articles\\\\tech\\\\Constraints_on_Rparity_Violation_from_Precision_Electroweak\\n__Measurements.txt'\n",
      "Enregistrement de l'article : Women Want to Learn Tech: Lessons from the Czechitas Education Project\n",
      "Enregistrement de l'article : A Review of Research on Civic Technology: Definitions, Theories, History\n",
      "  and Insights\n",
      "Erreur lors de l'enregistrement de A Review of Research on Civic Technology: Definitions, Theories, History\n",
      "  and Insights : [Errno 22] Invalid argument: 'corpus/scientific_articles\\\\tech\\\\A_Review_of_Research_on_Civic_Technology_Definitions_Theories_History\\n__and_Insights.txt'\n",
      "Récupération des articles scientifiques pour le mot-clé : biology\n",
      "Enregistrement de l'article : Borges Dilemma, Fundamental Laws, and Systems Biology\n",
      "Enregistrement de l'article : Quantum Biology at the Cellular Level - elements of the research program\n",
      "Enregistrement de l'article : Landscape Paradigms in Physics and Biology: Introduction and Overview\n",
      "Enregistrement de l'article : Mathematics at the eve of a historic transition in biology\n",
      "Enregistrement de l'article : G-quadruplexes and mRNA localization\n",
      "Récupération des articles scientifiques pour le mot-clé : climate change\n",
      "Enregistrement de l'article : The structure of the climate debate\n",
      "Enregistrement de l'article : Baumol's Climate Disease\n",
      "Enregistrement de l'article : You are right. I am ALARMED -- But by Climate Change Counter Movement\n",
      "Enregistrement de l'article : Climate Change Conspiracy Theories on Social Media\n",
      "Enregistrement de l'article : Hurricanes Increase Climate Change Conversations on Twitter\n",
      "Récupération des articles scientifiques pour le mot-clé : quantum computing\n",
      "Enregistrement de l'article : The Rise of Quantum Internet Computing\n",
      "Enregistrement de l'article : Unconventional Quantum Computing Devices\n",
      "Enregistrement de l'article : Geometrical perspective on quantum states and quantum computation\n",
      "Enregistrement de l'article : Quantum Computation and Quantum Information\n",
      "Enregistrement de l'article : Google Quantum AI's Quest for Error-Corrected Quantum Computers\n",
      "Les métadonnées des articles scientifiques ont été enregistrées dans all_documents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def save_article(keyword_path, abstract, title):\n",
    "    \"\"\"\n",
    "    Sauvegarde le contenu d'un article scientifique dans un fichier texte.\n",
    "\n",
    "    Args:\n",
    "        keyword_path (str): Répertoire où sauvegarder l'article.\n",
    "        abstract (str): Résumé de l'article.\n",
    "        title (str): Titre de l'article.\n",
    "\n",
    "    Returns:\n",
    "        str: Chemin complet du fichier enregistré.\n",
    "    \"\"\"\n",
    "    # Nettoyage du titre pour éviter les caractères non valides dans les noms de fichiers\n",
    "    safe_title = re.sub(r'[^\\w\\s]', '', title).replace(\" \", \"_\")\n",
    "    file_name = f\"{safe_title}.txt\"\n",
    "    file_path = os.path.join(keyword_path, file_name)\n",
    "\n",
    "    # Écrire le résumé dans un fichier texte\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(abstract)\n",
    "\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def fetch_scientific_articles(keywords, save_path=\"corpus/scientific_articles\", metadata_file=\"all_documents.json\"):\n",
    "    \"\"\"\n",
    "    Récupère des articles scientifiques courts à partir de l'API ArXiv, extrait leur contenu, et enregistre les métadonnées.\n",
    "\n",
    "    Args:\n",
    "        keywords (list): Liste de mots-clés pour filtrer les articles (par ex. ['AI', 'biology', 'climate']).\n",
    "        save_path (str): Répertoire où sauvegarder les articles scientifiques extraits.\n",
    "        metadata_file (str): Fichier JSON pour sauvegarder les métadonnées des articles scientifiques.\n",
    "    \"\"\"\n",
    "    # Charger ou initialiser les métadonnées\n",
    "    if os.path.exists(metadata_file):\n",
    "        with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = []\n",
    "\n",
    "    article_id = len(metadata) + 1  # ID unique pour chaque article\n",
    "\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "    for keyword in keywords:\n",
    "        print(f\"Récupération des articles scientifiques pour le mot-clé : {keyword}\")\n",
    "        params = {\n",
    "            \"search_query\": f\"all:{keyword}\",\n",
    "            \"start\": 0,\n",
    "            \"max_results\": 5\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Échec lors de la récupération des articles pour {keyword} : {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        # Parser la réponse XML avec BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"xml\")\n",
    "        entries = soup.find_all(\"entry\")\n",
    "\n",
    "        keyword_path = os.path.join(save_path, keyword)\n",
    "        os.makedirs(keyword_path, exist_ok=True)\n",
    "\n",
    "        for article in entries:\n",
    "            title = article.find(\"title\").get_text(strip=True)\n",
    "            authors = \", \".join([author.find(\"name\").get_text(strip=True) for author in article.find_all(\"author\")])\n",
    "            abstract = article.find(\"summary\").get_text(strip=True)\n",
    "            paper_url = article.find(\"id\").get_text(strip=True)\n",
    "\n",
    "            if not abstract or not paper_url:\n",
    "                print(f\"Article {title} ignoré : Résumé ou URL manquant.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                print(f\"Enregistrement de l'article : {title}\")\n",
    "                file_path = save_article(keyword_path, abstract, title)\n",
    "\n",
    "                metadata.append({\n",
    "                    \"id\": article_id,\n",
    "                    \"title\": title,\n",
    "                    \"author\": authors or \"Auteur inconnu\",\n",
    "                    \"type\": \"article scientifique\",\n",
    "                    \"categorie\": keyword,\n",
    "                    \"file_path\": file_path,\n",
    "                    \"url\": paper_url\n",
    "                })\n",
    "                article_id += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'enregistrement de {title} : {e}\")\n",
    "\n",
    "    # Sauvegarder les métadonnées dans un fichier JSON\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Les métadonnées des articles scientifiques ont été enregistrées dans {metadata_file}\")\n",
    "\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    KEYWORDS = [\"tech\", \"biology\", \"climate change\", \"quantum computing\"]\n",
    "    fetch_scientific_articles(KEYWORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing des docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les contenus prétraités et le vocabulaire ont été enregistrés dans le fichier : preprocessed_content.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import collections\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Charger le tokenizer de DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def read_documents(json_file_path):\n",
    "    \"\"\"\n",
    "    Lit le fichier JSON, extrait les ids et les chemins des fichiers, lit le contenu de chaque fichier,\n",
    "    et retourne un dictionnaire associant chaque id à son contenu.\n",
    "\n",
    "    :param json_file_path: Chemin du fichier JSON contenant les métadonnées des documents (id, chemin du fichier).\n",
    "    :return: Un dictionnaire {id: contenu du document}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "            documents = json.load(json_file)\n",
    "        \n",
    "        doc_contents = {}\n",
    "        for doc in documents:\n",
    "            doc_id = doc.get(\"id\")\n",
    "            file_path = doc.get(\"file_path\")\n",
    "            # Vérification de l'existence du fichier et des informations nécessaires\n",
    "            if doc_id and file_path and os.path.exists(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                doc_contents[doc_id] = content\n",
    "            else:\n",
    "                print(f\"Fichier introuvable ou informations manquantes pour l'id {doc_id}: {file_path}\")\n",
    "        return doc_contents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la lecture des documents : {e}\")\n",
    "        return {}\n",
    "\n",
    "def preprocess_documents(documents_content):\n",
    "    \"\"\"\n",
    "    Applique le prétraitement aux documents :\n",
    "    - Tokenisation (utilisation de DistilBERT)\n",
    "    - Construction du vocabulaire global à partir des tokens\n",
    "\n",
    "    :param documents_content: Dictionnaire {id: contenu brut des documents}.\n",
    "    :return: Dictionnaire avec le contenu prétraité et le vocabulaire global sous forme de Counter.\n",
    "    \"\"\"\n",
    "    vocabulary = collections.Counter()\n",
    "    preprocessed_content = {}\n",
    "\n",
    "    for doc_id, content in documents_content.items():\n",
    "        # Tokenisation du contenu du document avec gestion de la longueur des séquences\n",
    "        tokens = tokenizer(content, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        # Enregistrer les tokens sous forme de dictionnaire avec input_ids et attention_mask\n",
    "        preprocessed_content[doc_id] = {\n",
    "            'input_ids': tokens['input_ids'].squeeze().tolist(),  # Convertir en liste simple\n",
    "            'attention_mask': tokens['attention_mask'].squeeze().tolist()  # Convertir en liste simple\n",
    "        }\n",
    "        \n",
    "        # Mise à jour du vocabulaire global\n",
    "        vocabulary.update(tokens['input_ids'].squeeze().tolist())\n",
    "\n",
    "    return preprocessed_content, vocabulary\n",
    "\n",
    "def save_preprocessed_data(preprocessed_content, vocabulary, output_file=\"preprocessed_content.json\"):\n",
    "    \"\"\"\n",
    "    Sauvegarde le contenu prétraité et le vocabulaire dans un fichier JSON.\n",
    "\n",
    "    :param preprocessed_content: Dictionnaire {id: contenu prétraité} (tokens).\n",
    "    :param vocabulary: Vocabulaire global sous forme de `collections.Counter` (liste des ids).\n",
    "    :param output_file: Chemin du fichier de sortie pour enregistrer les données.\n",
    "    \"\"\"\n",
    "    data_to_save = {\n",
    "        \"preprocessed_content\": preprocessed_content,\n",
    "        \"vocabulary\": list(vocabulary.keys())  # Utiliser les clés du Counter comme vocabulaire\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data_to_save, json_file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Les contenus prétraités et le vocabulaire ont été enregistrés dans le fichier : {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la sauvegarde des données : {e}\")\n",
    "\n",
    "# Application principale\n",
    "if __name__ == \"__main__\":\n",
    "    json_file_path = \"all_documents.json\"\n",
    "    documents_content = read_documents(json_file_path)\n",
    "\n",
    "    if documents_content:\n",
    "        preprocessed_content, vocabulary = preprocess_documents(documents_content)\n",
    "        save_preprocessed_data(preprocessed_content, vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation des documents sous forme de vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les vecteurs DistilBERT des documents ont été enregistrés dans le fichier distilbert_vectors.json.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Charger le modèle et le tokenizer DistilBERT\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def load_preprocessed_content(input_file=\"preprocessed_content.json\"):\n",
    "    \"\"\"\n",
    "    Charge le contenu prétraité et le vocabulaire depuis un fichier JSON.\n",
    "\n",
    "    :param input_file: Nom du fichier JSON contenant les données prétraitées et le vocabulaire.\n",
    "    :return: Tuple (preprocessed_content, vocabulary) contenant les données prétraitées et le vocabulaire.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        preprocessed_content = data.get(\"preprocessed_content\", {})\n",
    "        vocabulary = data.get(\"vocabulary\", [])\n",
    "        return preprocessed_content, vocabulary\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur : Le fichier {input_file} n'a pas été trouvé.\")\n",
    "        return {}, []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Erreur : Le fichier {input_file} n'est pas un fichier JSON valide.\")\n",
    "        return {}, []\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la lecture du fichier : {e}\")\n",
    "        return {}, []\n",
    "\n",
    "def create_distilbert_embeddings(doc_contents, output_file=\"distilbert_vectors.json\"):\n",
    "    \"\"\"\n",
    "    Utilise DistilBERT pour calculer des embeddings pour chaque document en moyennant les embeddings des tokens.\n",
    "\n",
    "    :param doc_contents: Dictionnaire {id: contenu prétraité des documents (tokens)}.\n",
    "    :param output_file: Nom du fichier JSON où les embeddings seront enregistrés.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not doc_contents:\n",
    "            print(\"Erreur : Aucun contenu de document fourni pour calculer les embeddings.\")\n",
    "            return\n",
    "\n",
    "        document_vectors = {}\n",
    "\n",
    "        for doc_id, tokens in doc_contents.items():\n",
    "            # Vérification que tokens est bien un dictionnaire avec 'input_ids' et 'attention_mask'\n",
    "            input_ids = tokens['input_ids']\n",
    "            attention_mask = tokens['attention_mask']\n",
    "\n",
    "            # Calcul des embeddings avec le modèle DistilBERT\n",
    "            with torch.no_grad():\n",
    "                # Assurez-vous que les tokens contiennent input_ids et attention_mask\n",
    "                outputs = model(input_ids=torch.tensor([input_ids]),  # Ajouter une dimension batch\n",
    "                                attention_mask=torch.tensor([attention_mask]))  # Ajouter une dimension batch\n",
    "                embeddings = outputs.last_hidden_state.mean(dim=1)  # Moyenne des embeddings des tokens\n",
    "\n",
    "            # Ajout des embeddings du document au dictionnaire\n",
    "            document_vectors[doc_id] = embeddings.squeeze().tolist()\n",
    "\n",
    "        # Enregistrer les vecteurs dans un fichier JSON\n",
    "        with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(document_vectors, json_file, indent=4)\n",
    "\n",
    "        print(f\"Les vecteurs DistilBERT des documents ont été enregistrés dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors du calcul des embeddings : {e}\")\n",
    "\n",
    "# Application principale\n",
    "if __name__ == \"__main__\":\n",
    "    # Charger les contenus prétraités\n",
    "    preprocessed_content, vocabulary = load_preprocessed_content()\n",
    "\n",
    "    # Vérifier si des contenus ont été chargés avec succès\n",
    "    if preprocessed_content:\n",
    "        # Créer les vecteurs avec DistilBERT\n",
    "        create_distilbert_embeddings(preprocessed_content, output_file=\"distilbert_vectors.json\")\n",
    "    else:\n",
    "        print(\"Impossible de générer les vecteurs car aucun contenu prétraité n'a été chargé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le calcule de similarites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrice de similarité a été enregistrée dans le fichier document_similarity_matrix.csv.\n",
      "Les similarités triées ont été enregistrées dans le fichier sorted_document_similarities.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def compute_similarity_matrix(vector_file=\"distilbert_vectors.json\", output_file=\"document_similarity_matrix.csv\"):\n",
    "    \"\"\"\n",
    "    Calcule la similarité cosinus entre tous les documents et enregistre les résultats dans un fichier CSV sous forme de matrice.\n",
    "\n",
    "    :param vector_file: Nom du fichier JSON contenant les vecteurs des documents.\n",
    "    :param output_file: Nom du fichier CSV pour enregistrer la matrice des similarités.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger les vecteurs depuis le fichier JSON\n",
    "        with open(vector_file, 'r', encoding='utf-8') as json_file:\n",
    "            document_vectors = json.load(json_file)\n",
    "        \n",
    "        # Extraire les IDs et les vecteurs\n",
    "        doc_ids = list(document_vectors.keys())\n",
    "        vectors = np.array([document_vectors[doc_id] for doc_id in doc_ids])\n",
    "        \n",
    "        # Calculer la matrice de similarité cosinus\n",
    "        similarity_matrix = cosine_similarity(vectors)\n",
    "        \n",
    "        # Enregistrer la matrice dans un fichier CSV\n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Écrire l'en-tête (IDs des documents)\n",
    "            writer.writerow([\"\"] + doc_ids)\n",
    "            \n",
    "            # Écrire chaque ligne (ID + similarités)\n",
    "            for i, doc_id in enumerate(doc_ids):\n",
    "                writer.writerow([doc_id] + similarity_matrix[i].tolist())\n",
    "        \n",
    "        print(f\"La matrice de similarité a été enregistrée dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "\n",
    "def save_sorted_similarities_from_matrix(matrix_file=\"document_similarity_matrix.csv\", output_file=\"sorted_document_similarities.csv\"):\n",
    "    \"\"\"\n",
    "    Enregistre dans un fichier CSV les IDs des documents les plus similaires pour chaque document,\n",
    "    triés par similarité décroissante, à partir d'une matrice de similarité déjà calculée.\n",
    "\n",
    "    :param matrix_file: Nom du fichier CSV contenant la matrice de similarité.\n",
    "    :param output_file: Nom du fichier CSV pour enregistrer les similarités triées (seulement les IDs).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger la matrice de similarité depuis le fichier CSV\n",
    "        with open(matrix_file, 'r', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            rows = list(reader)\n",
    "        \n",
    "        # Extraire les IDs des documents (en-tête)\n",
    "        doc_ids = rows[0][1:]\n",
    "        \n",
    "        # Préparer les similarités triées (seulement les IDs)\n",
    "        sorted_similarities = []\n",
    "        for i, row in enumerate(rows[1:]):\n",
    "            doc_id = row[0]\n",
    "            similarities = [(doc_ids[j], float(row[j + 1])) for j in range(len(doc_ids)) if i != j]\n",
    "            \n",
    "            # Trier les similarités par ordre décroissant et récupérer uniquement les IDs\n",
    "            sorted_doc_ids = [sim_doc_id for sim_doc_id, _ in sorted(similarities, key=lambda x: x[1], reverse=True)]\n",
    "            sorted_similarities.append((doc_id, sorted_doc_ids))\n",
    "        \n",
    "        # Enregistrer les IDs triés dans un fichier CSV\n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Écrire l'en-tête (Document + Liste d'IDs des documents similaires)\n",
    "            header = [\"Document\"] + [f\"Similar Doc {i+1}\" for i in range(len(doc_ids) - 1)]\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            # Écrire les similarités triées pour chaque document\n",
    "            for doc_id, sorted_doc_ids in sorted_similarities:\n",
    "                # Compléter avec des colonnes vides si moins de documents similaires\n",
    "                row = [doc_id] + sorted_doc_ids + [''] * (len(doc_ids) - 1 - len(sorted_doc_ids))\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"Les similarités triées ont été enregistrées dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    compute_similarity_matrix(vector_file=\"distilbert_vectors.json\", output_file=\"document_similarity_matrix.csv\")\n",
    "    save_sorted_similarities_from_matrix(\n",
    "        matrix_file=\"document_similarity_matrix.csv\",\n",
    "        output_file=\"sorted_document_similarities.csv\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
