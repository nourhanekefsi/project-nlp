{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing des docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les contenus prétraités et le vocabulaire ont été enregistrés dans le fichier :preprocessed_content.json.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "\n",
    "def read_documents(json_file_path):\n",
    "    \"\"\"\n",
    "    Lit le fichier JSON, extrait les ids et les chemins des fichiers, lit le contenu de chaque fichier,\n",
    "    et retourne un dictionnaire associant chaque id à son contenu.\n",
    "\n",
    "    :param json_file_path: Chemin du fichier JSON contenant les métadonnées.\n",
    "    :return: Un dictionnaire {id: contenu}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lire le fichier JSON\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "            documents = json.load(json_file)\n",
    "        \n",
    "        # Initialiser le dictionnaire pour les résultats\n",
    "        doc_contents = {}\n",
    "\n",
    "        # Parcourir les documents\n",
    "        for doc in documents:\n",
    "            doc_id = doc.get(\"id\")\n",
    "            file_path = doc.get(\"file_path\")\n",
    "            \n",
    "            # Vérifier si le chemin du fichier est valide\n",
    "            if doc_id and file_path and os.path.exists(file_path):\n",
    "                # Lire le contenu du fichier\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                doc_contents[doc_id] = content\n",
    "            else:\n",
    "                print(f\"Fichier introuvable ou informations manquantes pour l'id {doc_id}: {file_path}\")\n",
    "        \n",
    "        return doc_contents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def pretraitement(document_content, vocabulary):\n",
    "    \"\"\"\n",
    "    Prend en entrée le contenu des fichiers et applique le prétraitement :\n",
    "    - Tokenisation\n",
    "    - Suppression des stopwords\n",
    "    - Conversion en minuscules\n",
    "    - Lemmatisation\n",
    "    - Stemming\n",
    "    \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('french')\n",
    "    \n",
    "    # Instanciation des objets WordNetLemmatizer \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    vocabulary = collections.Counter()\n",
    "    for file, file_content in documents_content.items():\n",
    "        file_content = re.sub('[0-9]+',' ', file_content)\n",
    "        # Tokenisation des mots dans le contenu du fichier\n",
    "        tokens = nltk.regexp_tokenize(file_content, pattern=r'\\w+')  # Tokenisation avec le bon pattern\n",
    "        \n",
    "        # Suppression des stopwords et conversion en minuscules\n",
    "        tokens = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "        \n",
    "        # Lemmatisation\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Mise à jour du contenu du fichier traité dans le dictionnaire\n",
    "        documents_content[file] = tokens\n",
    "        \n",
    "        # Ajout des mots au vocabulaire global (flatten la liste)\n",
    "        vocabulary.update(tokens)\n",
    "        # Sauvegarder dans un fichier JSON\n",
    "    data_to_save = {\n",
    "        \"preprocessed_content\": documents_content,\n",
    "        \"vocabulary\": list(vocabulary.keys())\n",
    "    }\n",
    "        \n",
    "    with open('preprocessed_content.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data_to_save, json_file, indent=4)\n",
    "    print(f\"Les contenus prétraités et le vocabulaire ont été enregistrés dans le fichier :preprocessed_content.json.\")\n",
    "\n",
    "            \n",
    "\n",
    "# Application\n",
    "\n",
    "json_file_path = \"all_documents.json\"\n",
    "documents_content = read_documents(json_file_path)\n",
    "vocabulary = []\n",
    "pretraitement(documents_content, vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation des documents sous forme de vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle Doc2Vec a été sauvegardé sous le nom 'doc2vec_model.model'.\n",
      "Les vecteurs des documents ont été enregistrés dans le fichier document_vectors.json.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import json\n",
    "\n",
    "def load_preprocessed_content(input_file=\"preprocessed_content.json\"):\n",
    "    \"\"\"\n",
    "    Charge le contenu prétraité et le vocabulaire depuis un fichier JSON.\n",
    "\n",
    "    :param input_file: Nom du fichier JSON contenant les données.\n",
    "    :return: Tuple (preprocessed_content, vocabulary).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        \n",
    "        preprocessed_content = data.get(\"preprocessed_content\", {})\n",
    "        vocabulary = data.get(\"vocabulary\", [])\n",
    "        return preprocessed_content, vocabulary\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur : Le fichier {input_file} n'a pas été trouvé.\")\n",
    "        return {}, []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Erreur : Le fichier {input_file} n'est pas un fichier JSON valide.\")\n",
    "        return {}, []\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite lors de la lecture du fichier : {e}\")\n",
    "        return {}, []\n",
    "\n",
    "def create_doc2vec_model(doc_contents, output_file=\"document_vectors.json\", vector_size=200, epochs=20):\n",
    "    \"\"\"\n",
    "    Crée un modèle Doc2Vec, calcule le vecteur de chaque document et enregistre les vecteurs dans un fichier JSON.\n",
    "\n",
    "    :param doc_contents: Dictionnaire {id: contenu du document}.\n",
    "    :param output_file: Nom du fichier pour enregistrer les vecteurs.\n",
    "    :param vector_size: Taille des vecteurs générés.\n",
    "    :param epochs: Nombre d'époques pour entraîner le modèle.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Vérification si doc_contents est vide\n",
    "        if not doc_contents:\n",
    "            print(\"Erreur : Aucun contenu de document fourni pour entraîner le modèle.\")\n",
    "            return\n",
    "        \n",
    "        # Préparer les données pour Doc2Vec\n",
    "        tagged_documents = [\n",
    "            TaggedDocument(words=content, tags=[doc_id]) \n",
    "            for doc_id, content in doc_contents.items()\n",
    "        ]\n",
    "        \n",
    "        # Initialiser et entraîner le modèle\n",
    "        model = Doc2Vec(vector_size=vector_size, min_count=1, epochs=epochs, workers=4)\n",
    "        model.build_vocab(tagged_documents)\n",
    "        model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        # Sauvegarder le modèle Doc2Vec\n",
    "        model.save(\"doc2vec_model.model\")\n",
    "        print(\"Le modèle Doc2Vec a été sauvegardé sous le nom 'doc2vec_model.model'.\")\n",
    "\n",
    "        # Calculer les vecteurs des documents\n",
    "        document_vectors = {doc_id: model.dv[doc_id].tolist() for doc_id in doc_contents.keys()}\n",
    "        \n",
    "        # Enregistrer les vecteurs dans un fichier JSON\n",
    "        with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(document_vectors, json_file, indent=4)\n",
    "        \n",
    "        print(f\"Les vecteurs des documents ont été enregistrés dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    # Charger les contenus prétraités\n",
    "    preprocessed_content, vocabulary = load_preprocessed_content()\n",
    "    \n",
    "    # Vérifier si des contenus ont été chargés avec succès\n",
    "    if preprocessed_content:\n",
    "        # Créer le modèle Doc2Vec et calculer les vecteurs\n",
    "        create_doc2vec_model(preprocessed_content, output_file=\"document_vectors.json\")\n",
    "    else:\n",
    "        print(\"Impossible de créer le modèle Doc2Vec car aucun contenu prétraité n'a été chargé.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le calcule de similarites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrice de similarité a été enregistrée dans le fichier document_similarity_matrix.csv.\n",
      "Les similarités triées ont été enregistrées dans le fichier sorted_document_similarities.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def compute_similarity_matrix(vector_file=\"document_vectors.json\", output_file=\"document_similarity_matrix.csv\"):\n",
    "    \"\"\"\n",
    "    Calcule la similarité cosinus entre tous les documents et enregistre les résultats dans un fichier CSV sous forme de matrice.\n",
    "\n",
    "    :param vector_file: Nom du fichier JSON contenant les vecteurs des documents.\n",
    "    :param output_file: Nom du fichier CSV pour enregistrer la matrice des similarités.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger les vecteurs depuis le fichier JSON\n",
    "        with open(vector_file, 'r', encoding='utf-8') as json_file:\n",
    "            document_vectors = json.load(json_file)\n",
    "        \n",
    "        # Extraire les IDs et les vecteurs\n",
    "        doc_ids = list(document_vectors.keys())\n",
    "        vectors = np.array([document_vectors[doc_id] for doc_id in doc_ids])\n",
    "        \n",
    "        # Calculer la matrice de similarité cosinus\n",
    "        similarity_matrix = cosine_similarity(vectors)\n",
    "        \n",
    "        # Enregistrer la matrice dans un fichier CSV\n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Écrire l'en-tête (IDs des documents)\n",
    "            writer.writerow([\"\"] + doc_ids)\n",
    "            \n",
    "            # Écrire chaque ligne (ID + similarités)\n",
    "            for i, doc_id in enumerate(doc_ids):\n",
    "                writer.writerow([doc_id] + similarity_matrix[i].tolist())\n",
    "        \n",
    "        print(f\"La matrice de similarité a été enregistrée dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "import csv\n",
    "\n",
    "def save_sorted_similarities_from_matrix(matrix_file=\"document_similarity_matrix.csv\", output_file=\"sorted_document_similarities.csv\"):\n",
    "    \"\"\"\n",
    "    Enregistre dans un fichier CSV les IDs des documents les plus similaires pour chaque document,\n",
    "    triés par similarité décroissante, à partir d'une matrice de similarité déjà calculée.\n",
    "\n",
    "    :param matrix_file: Nom du fichier CSV contenant la matrice de similarité.\n",
    "    :param output_file: Nom du fichier CSV pour enregistrer les similarités triées (seulement les IDs).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger la matrice de similarité depuis le fichier CSV\n",
    "        with open(matrix_file, 'r', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            rows = list(reader)\n",
    "        \n",
    "        # Extraire les IDs des documents (en-tête)\n",
    "        doc_ids = rows[0][1:]\n",
    "        \n",
    "        # Préparer les similarités triées (seulement les IDs)\n",
    "        sorted_similarities = []\n",
    "        for i, row in enumerate(rows[1:]):\n",
    "            doc_id = row[0]\n",
    "            similarities = [(doc_ids[j], float(row[j + 1])) for j in range(len(doc_ids)) if i != j]\n",
    "            \n",
    "            # Trier les similarités par ordre décroissant et récupérer uniquement les IDs\n",
    "            sorted_doc_ids = [sim_doc_id for sim_doc_id, _ in sorted(similarities, key=lambda x: x[1], reverse=True)]\n",
    "            sorted_similarities.append((doc_id, sorted_doc_ids))\n",
    "        \n",
    "        # Enregistrer les IDs triés dans un fichier CSV\n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Écrire l'en-tête (Document + Liste d'IDs des documents similaires)\n",
    "            header = [\"Document\"] + [f\"Similar Doc {i+1}\" for i in range(len(doc_ids) - 1)]\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            # Écrire les similarités triées pour chaque document\n",
    "            for doc_id, sorted_doc_ids in sorted_similarities:\n",
    "                # Compléter avec des colonnes vides si moins de documents similaires\n",
    "                row = [doc_id] + sorted_doc_ids + [''] * (len(doc_ids) - 1 - len(sorted_doc_ids))\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(f\"Les similarités triées ont été enregistrées dans le fichier {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "# Application\n",
    "if __name__ == \"__main__\":\n",
    "    compute_similarity_matrix(vector_file=\"document_vectors.json\", output_file=\"document_similarity_matrix.csv\")\n",
    "    save_sorted_similarities_from_matrix(\n",
    "        matrix_file=\"document_similarity_matrix.csv\",\n",
    "        output_file=\"sorted_document_similarities.csv\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
